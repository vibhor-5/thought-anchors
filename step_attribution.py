import multiprocessing
# Set the start method to 'spawn' for CUDA compatibility
if __name__ == '__main__':
    multiprocessing.set_start_method('spawn', force=True)

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from utils import split_solution_into_chunks
import gc
import argparse
import multiprocessing
from functools import partial
import matplotlib.colors as mcolors

# Define consistent category colors (copied from new_plots.py)
CATEGORY_COLORS = {
    'Active Computation': '#34A853', 
    'Fact Retrieval': '#FBBC05', 
    'Final Answer Emission': '#795548', 
    'Plan Generation': '#EA4335', 
    'Problem Setup': '#4285F4', 
    'Result Consolidation': '#00BCD4', 
    'Self Checking': '#FF9800',
    'Uncertainty Management': '#9C27B0'
}

# Define font size for consistency
FONT_SIZE = 20
plt.rcParams.update({
    'font.size': FONT_SIZE + 4,
    'axes.titlesize': FONT_SIZE + 12,
    'axes.labelsize': FONT_SIZE + 12,
    'xtick.labelsize': FONT_SIZE + 5,
    'ytick.labelsize': FONT_SIZE + 5,
    'legend.fontsize': FONT_SIZE + 4
})
FIGSIZE = (16, 14)
plt.rcParams.update({
    'axes.labelpad': 20,        # Padding for axis labels
    'axes.titlepad': 20,        # Padding for plot titles
    'axes.spines.top': False,   # Hide top spine
    'axes.spines.right': False, # Hide right spine
})

CMAP = "RdBu"

def load_problem(problem_dir: Path) -> Dict:
    """Load problem from problem directory."""
    problem_file = problem_dir / "problem.json"
    if problem_file.exists():
        with open(problem_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def load_base_solution(problem_dir: Path) -> Dict:
    """Load base solution from problem directory."""
    base_solution_file = problem_dir / "base_solution.json"
    if base_solution_file.exists():
        with open(base_solution_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def load_chunks(problem_dir: Path) -> List[str]:
    """Load chunks from problem directory."""
    chunks_file = problem_dir / "chunks_labeled.json"
    if chunks_file.exists():
        with open(chunks_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
            return chunks_data
    return []

def load_chunk_rollouts(problem_dir: Path, chunk_idx: int) -> List[Dict]:
    """Load rollouts for a specific chunk."""
    chunk_dir = problem_dir / f"chunk_{chunk_idx}"
    solutions_file = chunk_dir / "solutions.json"
    
    if solutions_file.exists():
        with open(solutions_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def extract_steps_from_rollout(rollout_text: str) -> List[str]:
    """Extract steps from a rollout text."""
    return split_solution_into_chunks(rollout_text)

def compute_embedding_similarity(text1: str, text2: str, model) -> float:
    """Compute cosine similarity between embeddings of two texts."""
    if not text1 or not text2:
        return 0.0
    
    # Get embeddings
    embedding1 = model.encode([text1])[0]
    embedding2 = model.encode([text2])[0]
    
    # Compute cosine similarity
    similarity = cosine_similarity([embedding1], [embedding2])[0][0]
    
    return float(similarity)

def find_best_matches_fully_batched(target_step: str, all_rollout_steps: List[List[str]], model, threshold: float = 0.7) -> List[Tuple[int, float]]:
    """
    Find the best matching step in each rollout for the target_step using full batching.
    
    Args:
        target_step: The step to find a match for
        all_rollout_steps: List of lists, where each inner list contains steps from one rollout
        model: Sentence embedding model
        threshold: Minimum similarity threshold to consider a match
        
    Returns:
        List of tuples (best_match_index, similarity_score) for each rollout
    """
    if not all_rollout_steps:
        return []
    
    # Flatten all steps from all rollouts and keep track of which rollout they belong to
    all_steps = []
    rollout_indices = []
    step_indices = []
    
    for rollout_idx, rollout_steps in enumerate(all_rollout_steps):
        for step_idx, step in enumerate(rollout_steps):
            all_steps.append(step)
            rollout_indices.append(rollout_idx)
            step_indices.append(step_idx)
    
    if not all_steps:
        return [(-1, 0.0)] * len(all_rollout_steps)
    
    # Get embedding for target step and all steps in a single batch
    all_embeddings = model.encode([target_step] + all_steps)
    target_embedding = all_embeddings[0]
    step_embeddings = all_embeddings[1:]
    
    # Compute similarities with target
    all_similarities = cosine_similarity([target_embedding], step_embeddings)[0]
    
    # Initialize results with default values
    best_matches = [(-1, 0.0)] * len(all_rollout_steps)
    
    # Group by rollout and find best match for each
    for i, (rollout_idx, step_idx, similarity) in enumerate(zip(rollout_indices, step_indices, all_similarities)):
        if similarity >= threshold and (best_matches[rollout_idx][0] == -1 or similarity > best_matches[rollout_idx][1]):
            best_matches[rollout_idx] = (step_idx, float(similarity))
    
    return best_matches

def process_chunk_pair(args):
    """
    Process a pair of chunks to compute importance.
    This function is designed to be used with multiprocessing.
    
    Args:
        args: Tuple containing (i, j, target_step, problem_dir, embedding_model, similarity_threshold)
        
    Returns:
        Tuple of (i, j, importance)
    """
    i, j, target_step, problem_dir, embedding_model, similarity_threshold = args
    
    # Get rollouts where step i was kept (resampling started at i+1)
    include_i_rollouts = load_chunk_rollouts(problem_dir, i+1)
    
    # Get rollouts where step i was removed (resampling started at i)
    exclude_i_rollouts = load_chunk_rollouts(problem_dir, i)
    
    # Filter for valid rollouts (has rollout text)
    include_i_rollouts = [r for r in include_i_rollouts if 'rollout' in r and r['rollout']]
    exclude_i_rollouts = [r for r in exclude_i_rollouts if 'rollout' in r and r['rollout']]
    
    if not include_i_rollouts or not exclude_i_rollouts:
        return i, j, 0.0
    
    # Extract steps from all rollouts
    include_i_steps = [extract_steps_from_rollout(rollout['rollout']) for rollout in include_i_rollouts]
    exclude_i_steps = [extract_steps_from_rollout(rollout['rollout']) for rollout in exclude_i_rollouts]
    
    # Process all rollouts where step i was kept
    include_i_matches = find_best_matches_fully_batched(target_step, include_i_steps, embedding_model, similarity_threshold)
    
    # Process all rollouts where step i was removed
    exclude_i_matches = find_best_matches_fully_batched(target_step, exclude_i_steps, embedding_model, similarity_threshold)
    
    # Calculate average similarity for both cases
    include_positives = [sim for _, sim in include_i_matches if sim > 0]
    exclude_positives = [sim for _, sim in exclude_i_matches if sim > 0]

    include_i_avg_similarity = np.mean(include_positives) if include_positives else 0.0
    exclude_i_avg_similarity = np.mean(exclude_positives) if exclude_positives else 0.0

    # Calculate match rate for both cases
    include_i_match_rate = len(include_positives) / len(include_i_matches) if include_i_matches else 0
    exclude_i_match_rate = len(exclude_positives) / len(exclude_i_matches) if exclude_i_matches else 0

    # Calculate importance as difference in match rate and similarity
    importance = include_i_match_rate - exclude_i_match_rate # NOTE: For now, we only use match rate difference
    """
    if include_positives and exclude_positives:  # Only add similarity difference if both have matches
        importance += 0.5 * (include_i_avg_similarity - exclude_i_avg_similarity)
    elif include_positives:  # Only include_i has matches
        importance += 0.5 * include_i_avg_similarity
    elif exclude_positives:  # Only exclude_i has matches
        importance -= 0.5 * exclude_i_avg_similarity
    """
    return i, j, importance

def compute_step_importance_matrix(
    problem_dir: Path,
    embedding_model,
    similarity_threshold: float = 0.7,
    max_chunks: int = 50,
    use_cache: bool = True,
    n_processes: int = None,
    output_dir: Path = None
) -> Tuple[np.ndarray, List[str]]:
    """
    Compute the step importance matrix for a problem.
    
    Args:
        problem_dir: Path to problem directory
        embedding_model: Sentence embedding model
        similarity_threshold: Threshold for considering steps similar
        max_chunks: Maximum number of chunks to analyze
        use_cache: Whether to use cached results
        n_processes: Number of processes to use for parallel computation
        output_dir: Directory to save cache files (if None, uses problem_dir)
        
    Returns:
        Tuple of (importance_matrix, chunk_texts)
        importance_matrix[i, j] represents the causal importance of step i on step j
    """
    # Determine cache directory location - use output_dir if provided
    if output_dir is not None:
        cache_dir = output_dir / "attribution_cache"
    else:
        raise ValueError("output_dir must be provided")
    
    cache_dir.mkdir(exist_ok=True, parents=True)
    cache_file = cache_dir / f"step_importance_matrix_t{similarity_threshold}.npz"
    
    if use_cache and cache_file.exists():
        print(f"Loading cached step importance matrix from {cache_file}")
        data = np.load(cache_file, allow_pickle=True)
        return data['matrix'], data['chunk_texts']
    
    # Load base solution and chunks
    base_solution = load_base_solution(problem_dir)
    original_chunks = load_chunks(problem_dir)
    
    if not original_chunks:
        print(f"No chunks found for problem {problem_dir.name}")
        return np.array([]), []
    
    # Limit number of chunks to analyze
    original_chunks = original_chunks[:max_chunks]
    num_chunks = len(original_chunks)
    
    print(f"Analyzing {num_chunks} chunks for problem {problem_dir.name}")
    
    # Initialize importance matrix
    # importance_matrix[i, j] represents the causal importance of step i on step j
    importance_matrix = np.zeros((num_chunks, num_chunks))
    
    # Pre-compute embeddings for all original chunks
    print("Computing embeddings for all original chunks...")
    original_chunk_texts = [chunk['chunk'] for chunk in original_chunks]
    original_embeddings = embedding_model.encode(original_chunk_texts)
    
    # Pre-load all rollouts for all chunks up front to avoid repeated disk I/O
    print("Loading all rollouts...")
    all_rollouts = {}
    for chunk_idx in range(num_chunks):
        rollouts = load_chunk_rollouts(problem_dir, chunk_idx)
        all_rollouts[chunk_idx] = [r for r in rollouts if 'rollout' in r and r['rollout']]
    
    # Pre-compute all rollout steps and their embeddings
    print("Pre-computing all rollout steps and embeddings...")
    chunk_rollout_steps = {}
    chunk_rollout_embeddings = {}
    
    for chunk_idx, rollouts in tqdm(all_rollouts.items(), desc="Pre-computing rollout steps and embeddings"):
        # Extract steps from all rollouts for this chunk
        all_steps = []
        step_indices = []  # to track which rollout and step within rollout
        rollout_indices = []
        
        for rollout_idx, rollout in enumerate(rollouts):
            steps = extract_steps_from_rollout(rollout['rollout'])
            for step_idx, step in enumerate(steps):
                all_steps.append(step)
                rollout_indices.append(rollout_idx)
                step_indices.append(step_idx)
        
        # Store step information
        chunk_rollout_steps[chunk_idx] = {
            'steps': all_steps,
            'rollout_indices': rollout_indices,
            'step_indices': step_indices,
            'num_rollouts': len(rollouts)
        }
        
        # Compute embeddings for all steps at once if there are steps
        if all_steps:
            embeddings = embedding_model.encode(all_steps)
            chunk_rollout_embeddings[chunk_idx] = embeddings
        else:
            chunk_rollout_embeddings[chunk_idx] = np.array([])
    
    # Process chunk pairs in one vectorized operation across the entire matrix
    print("Computing importance matrix...")
    
    # For each chunk i (source) and each chunk j (target) where j > i
    with tqdm(total=num_chunks*(num_chunks-1)//2, desc="Processing chunk pairs") as pbar:
        for i in range(num_chunks - 1):
            # Target chunks (all chunks after i)
            j_values = list(range(i+1, num_chunks))
            
            # Check if we have rollouts for both keeping and removing chunk i
            include_i_idx = i + 1  # Resampling started at i+1 (kept chunk i)
            exclude_i_idx = i      # Resampling started at i (removed chunk i)
            
            if include_i_idx not in chunk_rollout_steps or exclude_i_idx not in chunk_rollout_steps:
                pbar.update(len(j_values))
                continue
                
            if len(chunk_rollout_steps[include_i_idx]['steps']) == 0 or len(chunk_rollout_steps[exclude_i_idx]['steps']) == 0:
                pbar.update(len(j_values))
                continue
            
            # Get the rollout information
            include_steps_info = chunk_rollout_steps[include_i_idx]
            exclude_steps_info = chunk_rollout_steps[exclude_i_idx]
            include_embeddings = chunk_rollout_embeddings[include_i_idx]
            exclude_embeddings = chunk_rollout_embeddings[exclude_i_idx]
            
            # For each target chunk j
            for j in j_values:
                # Get the target embedding (chunk j)
                target_embedding = original_embeddings[j].reshape(1, -1)
                
                # Process include_i rollouts (chunk i was kept)
                include_similarities = cosine_similarity(target_embedding, include_embeddings)[0]
                include_matches = [(-1, 0.0)] * include_steps_info['num_rollouts']
                
                # Find best match for each rollout where chunk i was kept
                for step_idx, (rollout_idx, similarity) in enumerate(zip(include_steps_info['rollout_indices'], include_similarities)):
                    if similarity >= similarity_threshold and (include_matches[rollout_idx][0] == -1 or similarity > include_matches[rollout_idx][1]):
                        include_matches[rollout_idx] = (include_steps_info['step_indices'][step_idx], float(similarity))
                
                # Process exclude_i rollouts (chunk i was removed)
                exclude_similarities = cosine_similarity(target_embedding, exclude_embeddings)[0]
                exclude_matches = [(-1, 0.0)] * exclude_steps_info['num_rollouts']
                
                # Find best match for each rollout where chunk i was removed
                for step_idx, (rollout_idx, similarity) in enumerate(zip(exclude_steps_info['rollout_indices'], exclude_similarities)):
                    if similarity >= similarity_threshold and (exclude_matches[rollout_idx][0] == -1 or similarity > exclude_matches[rollout_idx][1]):
                        exclude_matches[rollout_idx] = (exclude_steps_info['step_indices'][step_idx], float(similarity))
                
                # Calculate match rates
                include_positives = [sim for _, sim in include_matches if sim > 0]
                exclude_positives = [sim for _, sim in exclude_matches if sim > 0]
                
                include_match_rate = len(include_positives) / len(include_matches) if include_matches else 0
                exclude_match_rate = len(exclude_positives) / len(exclude_matches) if exclude_matches else 0
                
                # Calculate importance as difference in match rate
                importance = include_match_rate - exclude_match_rate
                importance_matrix[i, j] = importance
                
                pbar.update(1)
    
    # Save to cache
    np.savez(cache_file, matrix=importance_matrix, chunk_texts=original_chunks)
    
    return importance_matrix, original_chunks

def plot_step_importance_matrix(
    importance_matrix: np.ndarray,
    chunk_texts: List[str],
    output_file: Path,
    problem_idx: str,
    max_text_length: int = 30,
    num_top_sentences: int = None
) -> None:
    """
    Plot the step importance matrix as a heatmap.
    
    Args:
        importance_matrix: Step importance matrix
        chunk_texts: List of chunk texts
        output_file: Path to save the plot
        problem_idx: Problem index for the title
        max_text_length: Maximum length of chunk text to display
        num_top_sentences: Number of top sentences to show in importance matrix (None = show all)
    """
    # Create shortened labels
    labels = []
    for i, text in enumerate(chunk_texts):
        if len(text) > max_text_length:
            text = text[:max_text_length] + "..."
        labels.append(f"{i}: {text}")
    
    # Create figure
    plt.figure(figsize=FIGSIZE)
    
    # Create the plot matrix (transposed)
    plot_matrix = importance_matrix.T.copy()
    
    # Create mask for upper triangle (where j <= i, meaning target step <= source step)
    # We want to mask these out since they don't represent meaningful causal relationships
    mask_upper = np.triu(np.ones_like(plot_matrix, dtype=bool))
    
    ax = sns.heatmap(
        plot_matrix,
        cmap=CMAP,
        norm=mcolors.Normalize(vmin=-max(abs(plot_matrix.min()), abs(plot_matrix.max())), vmax=max(abs(plot_matrix.min()), abs(plot_matrix.max()))),
        mask=mask_upper,  # Mask the upper triangle
        xticklabels=range(len(chunk_texts)),
        yticklabels=range(len(chunk_texts)),
        cbar_kws={"label": ""}
    )
    
    # Now manually color the masked (upper triangle) cells white
    # Get the current axis and draw white rectangles over the masked area
    for i in range(len(chunk_texts)):
        for j in range(len(chunk_texts)):
            if j <= i:  # Upper triangle (including diagonal)
                # Add a white rectangle at this position
                ax.add_patch(plt.Rectangle((i, j), 1, 1, fill=True, color='#FFFFFF', zorder=10))
    
    # Color the tick labels
    for i, (tick_label, color) in enumerate(zip(ax.get_xticklabels(), tick_colors)):
        tick_label.set_color(color)
    for i, (tick_label, color) in enumerate(zip(ax.get_yticklabels(), tick_colors)):
        tick_label.set_color(color)
    
    # Create title with additional info if filtering was applied
    title = f"Problem {problem_idx}: sentence-to-sentence importance matrix"
    if num_top_sentences is not None and len(chunk_texts) <= num_top_sentences:
        title += "" # f" (top {len(chunk_texts)} sentences)"
    
    plt.title(title)
    plt.xlabel("Target Step (j)")
    plt.ylabel("Source Step (i)")
    
    # Save figure
    plt.tight_layout()
    plt.savefig(output_file)
    plt.close()
    
    # Create a second figure with text labels for steps
    fig, ax = plt.subplots(figsize=(12, len(chunk_texts) * 0.4))
    ax.axis('off')
    
    # Add step texts
    for i, label in enumerate(labels):
        ax.text(0, 1 - (i / len(labels)), f"{label}")
    
    # Save step text figure
    plt.tight_layout()
    plt.savefig(output_file.parent / f"{output_file.stem}_steps.png")
    plt.close()

def get_function_tag_prefix(chunk):
    """Extract the first two uppercase letters from the first function tag."""
    if isinstance(chunk, dict) and 'function_tags' in chunk and chunk['function_tags']:
        # Get the first function tag
        first_tag = chunk['function_tags'][0]
        # Extract uppercase letters
        uppercase_letters = ''.join([word[0].upper() for word in first_tag.split('_')])
        # Return the first two (or fewer if only one exists)
        return uppercase_letters
    return ""

def get_category_from_abbreviation(abbreviation):
    """Map function tag abbreviation to full category name."""
    abbrev_to_category = {
        'AC': 'Active Computation',
        'FR': 'Fact Retrieval', 
        'FAE': 'Final Answer Emission',
        'PG': 'Plan Generation',
        'PS': 'Problem Setup',
        'RC': 'Result Consolidation',
        'SC': 'Self Checking',
        'UM': 'Uncertainty Management'
    }
    return abbrev_to_category.get(abbreviation.upper(), None)

def filter_chunks_by_excluded_tags(chunks, excluded_abbreviations):
    """Filter out chunks whose function tag abbreviations are in the excluded list."""
    if not excluded_abbreviations:
        return chunks, list(range(len(chunks)))
    
    excluded_set = set(abbrev.upper() for abbrev in excluded_abbreviations)
    filtered_chunks = []
    original_indices = []
    
    for i, chunk in enumerate(chunks):
        tag_prefix = get_function_tag_prefix(chunk)
        if tag_prefix not in excluded_set:
            filtered_chunks.append(chunk)
            original_indices.append(i)
    
    return filtered_chunks, original_indices

def analyze_step_attribution(
    problem_dirs: List[Path],
    output_dir: Path,
    similarity_threshold: float = 0.7,
    max_chunks: int = 50,
    use_cache: bool = True,
    n_processes: int = None,
    excluded_abbreviations: List[str] = None,
    num_top_sentences: int = None
) -> None:
    """
    Analyze step attribution for a list of problems.
    
    Args:
        problem_dirs: List of problem directories
        output_dir: Directory to save analysis results
        similarity_threshold: Threshold for considering steps similar
        max_chunks: Maximum number of chunks to analyze
        use_cache: Whether to use cached results
        n_processes: Number of processes to use for parallel computation
        excluded_abbreviations: List of function tag abbreviations to exclude from analysis
        num_top_sentences: Number of top sentences to show in importance matrix (None = show all)
    """
    # Create output directory
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # Create directories for correct and incorrect solutions
    correct_dir = output_dir / "correct_base_solution"
    incorrect_dir = output_dir / "incorrect_base_solution"
    correct_dir.mkdir(exist_ok=True, parents=True)
    incorrect_dir.mkdir(exist_ok=True, parents=True)
    
    # Load sentence embedding model
    print("Loading sentence embedding model...")
    model = SentenceTransformer('all-MiniLM-L6-v2').to('cuda:0')
    model.eval()
    
    # Process each problem
    for problem_dir in tqdm(problem_dirs, desc="Analyzing problems"):
        problem_idx = int(problem_dir.name.split('_')[-1])
        
        # Determine if this is a correct or incorrect solution
        is_correct = "correct_base_solution" in str(problem_dir) and "incorrect_base_solution" not in str(problem_dir)
        
        # Create problem-specific output directory in the appropriate location
        if is_correct:
            problem_output_dir = correct_dir / f"problem_{problem_idx}"
        else:
            problem_output_dir = incorrect_dir / f"problem_{problem_idx}"
        problem_output_dir.mkdir(exist_ok=True, parents=True)
        
        # Create plots directory
        plots_dir = problem_output_dir / "plots"
        plots_dir.mkdir(exist_ok=True, parents=True)
        
        # Create cache directory
        cache_dir = problem_output_dir / "cache"
        cache_dir.mkdir(exist_ok=True, parents=True)
        
        # Save base solution
        base_solution = load_base_solution(problem_dir)
        if base_solution:
            with open(problem_output_dir / "base_solution.json", 'w', encoding='utf-8') as f:
                json.dump(base_solution, f, indent=2)
        
        # Check if cached result exists
        cache_file = cache_dir / f"step_importance_matrix_t{similarity_threshold}.npz"
        
        if use_cache and cache_file.exists():
            print(f"Loading cached step importance matrix for problem {problem_idx}")
            data = np.load(cache_file, allow_pickle=True)
            importance_matrix = data['matrix']
            chunk_texts = data['chunk_texts'].tolist()
        else:
            # Compute step importance matrix
            importance_matrix, chunk_texts = compute_step_importance_matrix(
                problem_dir=problem_dir,
                embedding_model=model,
                similarity_threshold=similarity_threshold,
                max_chunks=max_chunks,
                use_cache=use_cache,
                n_processes=n_processes,
                output_dir=problem_output_dir  # Pass the output directory
            )
            
            # Save to cache
            np.savez(
                cache_file,
                matrix=importance_matrix,
                chunk_texts=np.array(chunk_texts, dtype=object)
            )
        
        # Apply top sentences selection first (before exclusion filtering)
        original_indices = list(range(len(chunk_texts)))  # Track original indices
        if num_top_sentences is not None:
            importance_matrix, chunk_texts, selected_indices = select_top_sentences(
                importance_matrix, chunk_texts, num_top_sentences
            )
            original_indices = selected_indices  # Update to reflect selection
            print(f"Selected top {len(chunk_texts)} sentences for problem {problem_idx}")
        
        # Filter chunks based on excluded abbreviations (after top selection)
        if excluded_abbreviations:
            filtered_chunk_texts, kept_indices = filter_chunks_by_excluded_tags(chunk_texts, excluded_abbreviations)
            
            # Filter the importance matrix to keep only the rows and columns for kept chunks
            if kept_indices:
                importance_matrix = importance_matrix[np.ix_(kept_indices, kept_indices)]
                chunk_texts = filtered_chunk_texts
                # Update original indices to reflect exclusion filtering
                original_indices = [original_indices[i] for i in kept_indices]
                print(f"Filtered to {len(chunk_texts)} chunks after excluding abbreviations: {excluded_abbreviations}")
            else:
                print(f"Warning: All chunks excluded for problem {problem_idx}, skipping plots")
                continue
        
        # Create axis labels with function tag prefixes using ORIGINAL indices
        x_labels = []
        y_labels = []
        tick_colors = []
        for i, chunk in enumerate(chunk_texts):
            tag_prefix = get_function_tag_prefix(chunk)
            # Use original index instead of current index i
            original_idx = original_indices[i]
            label = f"{original_idx}-{tag_prefix}" if tag_prefix else f"{original_idx}"
            x_labels.append(label)
            y_labels.append(label)
            
            # Get color for this category
            category = get_category_from_abbreviation(tag_prefix)
            color = CATEGORY_COLORS.get(category, '#000000') if category else '#000000'  # Default to black
            tick_colors.append(color)

        # Plot importance matrix
        plt.figure(figsize=FIGSIZE)
        
        # Create the plot matrix (transposed)
        plot_matrix = importance_matrix.T.copy()
        
        # Create mask for upper triangle (where j <= i, meaning target step <= source step)
        # We want to mask these out since they don't represent meaningful causal relationships
        mask_upper = np.triu(np.ones_like(plot_matrix, dtype=bool))
        
        ax = sns.heatmap(
            plot_matrix,
            cmap=CMAP,
            norm=mcolors.Normalize(vmin=-max(abs(plot_matrix.min()), abs(plot_matrix.max())), vmax=max(abs(plot_matrix.min()), abs(plot_matrix.max()))),
            mask=mask_upper,  # Mask the upper triangle
            xticklabels=x_labels,
            yticklabels=y_labels,
            cbar_kws={"label": ""}
        )
        
        # Now manually color the masked (upper triangle) cells white
        # Get the current axis and draw white rectangles over the masked area
        for i in range(len(chunk_texts)):
            for j in range(len(chunk_texts)):
                if j <= i:  # Upper triangle (including diagonal)
                    # Add a white rectangle at this position
                    ax.add_patch(plt.Rectangle((i, j), 1, 1, fill=True, color='#FFFFFF', zorder=10))
        
        # Color the tick labels
        for i, (tick_label, color) in enumerate(zip(ax.get_xticklabels(), tick_colors)):
            tick_label.set_color(color)
        for i, (tick_label, color) in enumerate(zip(ax.get_yticklabels(), tick_colors)):
            tick_label.set_color(color)
        
        # Create title with additional info if filtering was applied
        title = f"Problem {problem_idx}: sentence-to-sentence importance matrix"
        if num_top_sentences is not None and len(chunk_texts) <= num_top_sentences:
            title += "" # f" (top {len(chunk_texts)} sentences)"
        
        plt.title(title)
        plt.xlabel("Source step (i)")
        plt.ylabel("Target step (j)")
        
        plt.tight_layout()
        plt.savefig(plots_dir / f"step_importance_matrix.png", dpi=300)
        plt.close()
        
        # Plot outgoing importance for each step
        outgoing_importance = np.zeros(len(chunk_texts))
        for i in range(len(chunk_texts)):
            # Average importance of step i on all future steps
            future_steps = importance_matrix[i, i+1:]
            outgoing_importance[i] = np.mean(future_steps) if len(future_steps) > 0 else 0
        
        plt.figure(figsize=(12, 6))
        plt.bar(range(len(chunk_texts)), outgoing_importance)
        plt.title(f"Average Outgoing Importance for Problem {problem_idx}")
        plt.xlabel("Step Index")
        plt.ylabel("Average Importance on Future Steps")
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(plots_dir / f"outgoing_importance.png", dpi=300)
        plt.close()
        
        # Plot incoming importance for each step
        incoming_importance = np.zeros(len(chunk_texts))
        for j in range(len(chunk_texts)):
            # Average importance of all previous steps on step j
            previous_steps = importance_matrix[:j, j]
            incoming_importance[j] = np.mean(previous_steps) if len(previous_steps) > 0 else 0
        
        plt.figure(figsize=(12, 6))
        plt.bar(range(len(chunk_texts)), incoming_importance)
        plt.title(f"Average Incoming Importance for Problem {problem_idx}")
        plt.xlabel("Step Index")
        plt.ylabel("Average Importance from Previous Steps")
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(plots_dir / f"incoming_importance.png", dpi=300)
        plt.close()
        
        # Save chunk texts
        with open(problem_output_dir / "chunk_texts.json", 'w', encoding='utf-8') as f:
            json.dump(chunk_texts, f, indent=2)
        
        # Save importance scores in JSON format for easier access
        importance_data = []
        for i in range(len(chunk_texts)):
            # For each source step, create a list of its impacts on target steps
            target_impacts = []
            for j in range(len(chunk_texts)):
                if j > i:  # We only have importance scores for j > i
                    target_impacts.append({
                        "target_chunk_idx": j,
                        "importance_score": float(importance_matrix[i, j])
                    })
            
            # Create entry for this source step
            step_data = {
                "source_chunk_idx": i,
                "source_chunk_text": chunk_texts[i].get('chunk', str(chunk_texts[i])) if isinstance(chunk_texts[i], dict) else str(chunk_texts[i]),
                "target_impacts": target_impacts
            }
            importance_data.append(step_data)

        # Save to JSON file
        with open(problem_output_dir / "step_importance.json", 'w', encoding='utf-8') as f:
            json.dump(importance_data, f, indent=2)
        
        # Generate summary statistics
        summary = {
            "problem_idx": problem_idx,
            "num_chunks": len(chunk_texts),
            "avg_importance": float(np.mean(importance_matrix)),
            "max_importance": float(np.max(importance_matrix)),
            "min_importance": float(np.min(importance_matrix)),
        }
        
        # Find most influential steps (highest average outgoing importance)
        top_influential_indices = np.argsort(outgoing_importance)[::-1][:5]
        summary["top_influential_steps"] = [
            {
                "step_idx": int(idx),
                "step_text": chunk_texts[idx],
                "avg_outgoing_importance": float(outgoing_importance[idx])
            }
            for idx in top_influential_indices if outgoing_importance[idx] > 0
        ]
        
        # Find most dependent steps (highest average incoming importance)
        top_dependent_indices = np.argsort(incoming_importance)[::-1][:5]
        summary["top_dependent_steps"] = [
            {
                "step_idx": int(idx),
                "step_text": chunk_texts[idx],
                "avg_incoming_importance": float(incoming_importance[idx])
            }
            for idx in top_dependent_indices if incoming_importance[idx] > 0
        ]
        
        # Save summary
        with open(problem_output_dir / "summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2)
        
        # Free memory
        del importance_matrix
        gc.collect()
    
    print(f"Step attribution analysis complete. Results saved to {output_dir}")
    
def get_problem_dirs(analysis_dir: Path, correct_only: bool = True, limit: Optional[int] = None, include_problems: Optional[str] = None) -> List[Path]:
    """
    Get problem directories in the analysis directory that have complete chunk folders.
    
    Args:
        analysis_dir: Path to the analysis directory
        correct_only: Whether to only include correct solution directories
        limit: Optional limit on number of directories to return
        
    Returns:
        List of problem directory paths with complete chunk folders
    """
    # Determine which subdirectory to use based on correct_only
    print(f"Correct only: {correct_only}")
    subdir = "correct_base_solution" if correct_only else "incorrect_base_solution"
    print(f"Using {subdir} subdirectory")
    base_dir = analysis_dir / subdir
    
    if not base_dir.exists():
        print(f"Directory {base_dir} does not exist")
        return []
    
    # Get all problem directories
    all_problem_dirs = [d for d in base_dir.iterdir() if d.is_dir() and d.name.startswith("problem_")]
    
    if include_problems:
        all_problem_dirs = [d for d in all_problem_dirs if int(d.name.split('_')[-1]) in [int(p) for p in include_problems.split(',')]]
    
    # Filter for directories with complete chunk folders
    complete_problem_dirs = []
    incomplete_problem_dirs = []
    no_chunks_problem_dirs = []
    
    for problem_dir in all_problem_dirs:
        # Check if chunks_labeled.json exists
        chunks_file = problem_dir / "chunks_labeled.json"
        if not chunks_file.exists():
            no_chunks_problem_dirs.append(problem_dir)
            continue
        
        # Load chunks data
        with open(chunks_file, 'r', encoding='utf-8') as f:
            try:
                chunks_data = json.load(f)
            except json.JSONDecodeError:
                no_chunks_problem_dirs.append(problem_dir)
                continue
        
        # Check if all chunks have folders
        all_chunks_have_folders = True
        for chunk in chunks_data:
            chunk_idx = chunk.get("chunk_idx", -1)
            if chunk_idx == -1:
                all_chunks_have_folders = False
                break
                
            chunk_dir = problem_dir / f"chunk_{chunk_idx}"
            if not chunk_dir.exists() or not chunk_dir.is_dir():
                all_chunks_have_folders = False
                break
        
        if all_chunks_have_folders:
            complete_problem_dirs.append(problem_dir)
        else:
            incomplete_problem_dirs.append(problem_dir)
    
    # Print statistics
    total_problems = len(all_problem_dirs)
    complete_count = len(complete_problem_dirs)
    incomplete_count = len(incomplete_problem_dirs)
    no_chunks_count = len(no_chunks_problem_dirs)
    
    print(f"Found {total_problems} problem directories in {base_dir}")
    print(f"  - {complete_count} ({complete_count/total_problems*100:.1f}%) have complete chunk folders")
    print(f"  - {incomplete_count} ({incomplete_count/total_problems*100:.1f}%) have incomplete chunk folders")
    print(f"  - {no_chunks_count} ({no_chunks_count/total_problems*100:.1f}%) have no chunks data")
    
    # Apply limit if specified
    if limit and limit < len(complete_problem_dirs):
        complete_problem_dirs = complete_problem_dirs[:limit]
        print(f"Limited to {limit} problem directories")
    
    return complete_problem_dirs

def calculate_sentence_importance_scores(importance_matrix):
    """
    Calculate combined importance scores for each sentence based on outgoing and incoming importance.
    
    Args:
        importance_matrix: Step importance matrix where importance_matrix[i, j] is importance of step i on step j
        
    Returns:
        numpy array of combined importance scores for each sentence
    """
    num_sentences = importance_matrix.shape[0]
    
    # Calculate outgoing importance for each sentence
    outgoing_importance = np.zeros(num_sentences)
    for i in range(num_sentences):
        # Average importance of step i on all future steps
        future_steps = importance_matrix[i, i+1:]
        outgoing_importance[i] = np.mean(future_steps) if len(future_steps) > 0 else 0
    
    # Calculate incoming importance for each sentence
    incoming_importance = np.zeros(num_sentences)
    for j in range(num_sentences):
        # Average importance of all previous steps on step j
        previous_steps = importance_matrix[:j, j]
        incoming_importance[j] = np.mean(previous_steps) if len(previous_steps) > 0 else 0
    
    # Combined importance: sum of outgoing and incoming
    combined_importance = outgoing_importance + incoming_importance
    
    return combined_importance, outgoing_importance, incoming_importance

def select_top_sentences(importance_matrix, chunk_texts, num_top_sentences):
    """
    Select the top N most important sentences based on combined importance scores.
    
    Args:
        importance_matrix: Step importance matrix
        chunk_texts: List of chunk texts
        num_top_sentences: Number of top sentences to select
        
    Returns:
        Tuple of (filtered_matrix, filtered_chunks, selected_indices)
    """
    if num_top_sentences is None or num_top_sentences >= len(chunk_texts):
        return importance_matrix, chunk_texts, list(range(len(chunk_texts)))
    
    # Calculate importance scores
    combined_scores, outgoing_scores, incoming_scores = calculate_sentence_importance_scores(importance_matrix)
    
    # Get indices of top N sentences
    top_indices = np.argsort(combined_scores)[-num_top_sentences:][::-1]  # Descending order
    top_indices = sorted(top_indices)  # Sort to maintain original order
    
    print(f"Selected top {len(top_indices)} sentences with indices: {top_indices}")
    print(f"Combined importance scores: {combined_scores[top_indices]}")
    
    # Filter importance matrix to only include these sentences
    filtered_matrix = importance_matrix[np.ix_(top_indices, top_indices)]
    
    # Filter chunk texts
    filtered_chunks = [chunk_texts[i] for i in top_indices]
    
    return filtered_matrix, filtered_chunks, top_indices

def main():
    parser = argparse.ArgumentParser(description="Analyze step-to-step attribution in chain-of-thought reasoning")
    parser.add_argument("-ad", "--analysis_dir", type=str, default="math_rollouts/deepseek-r1-distill-qwen-14b/temperature_0.6_top_p_0.95", help="Directory containing analysis results")
    parser.add_argument("-od", "--output_dir", type=str, default="analysis/step_attribution", help="Directory to save analysis results")
    parser.add_argument("-st", "--similarity_threshold", type=float, default=0.8, help="Threshold for considering steps similar")
    parser.add_argument("-mc", "--max_chunks", type=int, default=None, help="Maximum number of chunks to analyze")
    parser.add_argument("-mp", "--max_problems", type=int, default=None, help="Maximum number of problems to analyze")
    parser.add_argument("-ip", "--include_problems", type=str, default=None, help="Comma-separated list of problem indices to include")
    parser.add_argument("-co", "--correct_only", action="store_true", help="Only analyze correct solutions, if not specified, only incorrect solutions will be analyzed")
    parser.add_argument("-nc", "--no_cache", action="store_true", default=True, help="Don't use cached results")
    parser.add_argument("-np", "--n_processes", type=int, default=1, help="Number of processes to use for parallel computation")
    parser.add_argument("-ex", "--exclude_tags", type=str, default=None, help="Comma-separated list of function tag abbreviations to exclude (e.g., 'SC,PS')")
    parser.add_argument("-nt", "--num_top_sentences", type=int, default=None, help="Number of top sentences to show in importance matrix (None = show all)")
    args = parser.parse_args()
    
    # Parse excluded abbreviations
    excluded_abbreviations = None
    if args.exclude_tags:
        excluded_abbreviations = [tag.strip() for tag in args.exclude_tags.split(',')]
        print(f"Excluding function tag abbreviations: {excluded_abbreviations}")
    
    # Set up directories
    analysis_dir = Path(args.analysis_dir)
    model_name = args.analysis_dir.split("/")[-2]
    output_dir = Path(args.output_dir) / model_name
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # Get problem directories
    problem_dirs = get_problem_dirs(analysis_dir, correct_only=args.correct_only, limit=args.max_problems, include_problems=args.include_problems)
    
    if not problem_dirs:
        print(f"No problem directories found in {analysis_dir}")
        return
    
    print(f"Found {len(problem_dirs)} problem directories")
    
    # Analyze step attribution
    analyze_step_attribution(
        problem_dirs=problem_dirs,
        output_dir=output_dir,
        similarity_threshold=args.similarity_threshold,
        max_chunks=args.max_chunks,
        use_cache=not args.no_cache,
        n_processes=args.n_processes,
        excluded_abbreviations=excluded_abbreviations,
        num_top_sentences=args.num_top_sentences
    )

if __name__ == "__main__":
    main()