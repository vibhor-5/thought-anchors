import argparse
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm
from matplotlib.patches import Ellipse
import matplotlib.transforms as transforms

parser = argparse.ArgumentParser(description='Plot analysis results')
parser.add_argument('-m', '--model', type=str, default="qwen-14b", choices=['qwen-14b', 'llama-8b'], help='Model to use for analysis')
parser.add_argument('-n', '--normalize', dest='normalize', action='store_false', help='Normalize importance values per problem')
parser.add_argument('-nn', '--no-normalize', dest='normalize', action='store_true', help='Use raw importance values without normalization')
parser.add_argument('-hh', '--hierarchical', action='store_true', default=True, help='Use hierarchical statistics (problem-level averages) for category analysis')
parser.add_argument('-nhh', '--no-hierarchical', dest='hierarchical', action='store_false', help='Use original statistics (chunk-level) for category analysis')
parser.add_argument('-ms', '--min-steps', type=int, default=0, help='Minimum number of steps (chunks) required for a problem to be included')
parser.set_defaults(normalize=True)
args = parser.parse_args()

IMPORTANCE_METRICS = ["resampling_importance_accuracy", "resampling_importance_kl", "counterfactual_importance_accuracy", "counterfactual_importance_kl", "forced_importance_accuracy", "forced_importance_kl"]

# Define consistent category colors to use across all plots
CATEGORY_COLORS = {
    'Active Computation': '#34A853', 
    'Fact Retrieval': '#FBBC05', 
    'Final Answer Emission': '#795548', 
    'Plan Generation': '#EA4335', 
    'Problem Setup': '#4285F4', 
    'Result Consolidation': '#00BCD4', 
    'Self Checking': '#FF9800',
    'Uncertainty Management': '#9C27B0'
}

categories_to_exclude = ['Problem Setup', 'Final Answer Emission', 'Self Checking']

# Check for required dependencies
try:
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.pipeline import make_pipeline
except ImportError:
    print("Warning: scikit-learn not found. Position-debiased plots will not be generated.")
    print("To install, run: pip install scikit-learn")
    HAS_SKLEARN = False
else:
    HAS_SKLEARN = True

# Set consistent font size for all plots
FONT_SIZE = 20
plt.rcParams.update({
    'font.size': FONT_SIZE,
    'axes.titlesize': FONT_SIZE + 4,
    'axes.labelsize': FONT_SIZE + 2,
    'xtick.labelsize': FONT_SIZE,
    'ytick.labelsize': FONT_SIZE,
    'legend.fontsize': FONT_SIZE - 1,
    'figure.titlesize': FONT_SIZE + 20
})
FIGSIZE = (9, 7)

plt.rcParams['axes.spines.top'] = False
plt.rcParams['axes.spines.right'] = False

plt.rcParams.update({
    'axes.labelpad': 20,        # Padding for axis labels
    'axes.titlepad': 20,        # Padding for plot titles
    'axes.spines.top': False,   # Hide top spine
    'axes.spines.right': False, # Hide right spine
})

def confidence_ellipse(x, y, ax, n_std=1.0, **kwargs):
    """
    Create a plot of the covariance confidence ellipse of *x* and *y*.
    
    Parameters
    ----------
    x, y : array-like, shape (n, )
        Input data.
    ax : matplotlib.axes.Axes
        The axes object to draw the ellipse into.
    n_std : float
        The number of standard deviations to determine the ellipse's radiuses.
    **kwargs
        Forwarded to `~matplotlib.patches.Ellipse`
    """
    if x.size != y.size:
        raise ValueError("x and y must be the same size")

    cov = np.cov(x, y)
    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])
    # Using a special case to obtain the eigenvalues of this
    # two-dimensional dataset.
    ell_radius_x = np.sqrt(1 + pearson)
    ell_radius_y = np.sqrt(1 - pearson)
    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, **kwargs)

    # Calculating the standard deviation of x from the square root of
    # the variance and multiplying with the given number of standard deviations.
    scale_x = np.sqrt(cov[0, 0]) * n_std
    mean_x = np.mean(x)

    # calculating the standard deviation of y ...
    scale_y = np.sqrt(cov[1, 1]) * n_std
    mean_y = np.mean(y)

    transf = transforms.Affine2D() \
        .rotate_deg(45) \
        .scale(scale_x, scale_y) \
        .translate(mean_x, mean_y)

    ellipse.set_transform(transf + ax.transData)
    return ax.add_patch(ellipse)

def collect_tag_data(rollouts_dir="math_rollouts/deepseek-r1-distill-qwen-14b/temperature_0.6_top_p_0.95"):
    """
    Collect data for each function tag: importance metrics and position information.
    
    Args:
        rollouts_dir: Directory containing rollout data
        
    Returns:
        DataFrame with aggregated tag data
    """
    print("Collecting data for function tags...")
    
    # Collect all tag data
    tag_data = []
    
    # Counters for filtering statistics
    total_chunks_with_tags = 0
    filtered_counterfactual_accuracy = 0
    filtered_counterfactual_kl = 0
    filtered_convergence = 0
    
    # Process correct and incorrect rollouts
    for is_correct in [True, False]:
        subdir = "correct_base_solution" if is_correct else "incorrect_base_solution"
        base_dir = Path(rollouts_dir) / subdir
        
        # Check if directory exists
        if not base_dir.exists():
            print(f"Warning: {base_dir} does not exist, skipping")
            continue
        
        # Find all problem directories
        problem_dirs = [d for d in base_dir.iterdir() if d.is_dir() and d.name.startswith("problem_")]
        
        for problem_dir in tqdm(problem_dirs, desc=f"Processing {'correct' if is_correct else 'incorrect'} problems"):
            # Extract problem number
            problem_number = problem_dir.name.split("_")[1]
            
            # Create problem ID with correctness indicator
            pn_ci = f"{problem_number}{'01' if is_correct else '00'}"
            
            # Convert to int for comparison
            pn_ci_int = int(pn_ci)
            
            # Path to chunks_labeled.json
            chunks_file = problem_dir / "chunks_labeled.json"
            
            if not chunks_file.exists():
                print(f"Warning: {chunks_file} does not exist, skipping")
                continue
            
            # Load labeled chunks
            try:
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    labeled_chunks = json.load(f)
            except Exception as e:
                print(f"Error reading {chunks_file}: {e}")
                continue
            
            # Get total number of chunks for normalizing position
            total_chunks = len(labeled_chunks)
            if total_chunks == 0:
                print(f"Warning: {chunks_file} has no chunks, skipping")
                continue
            
            # Detect convergence based on chunk accuracy
            # For each chunk, check if it and the next 4 chunks have accuracy > 0.99 or < 0.01
            converged_from_index = None
            
            # Sort chunks by chunk_idx to ensure proper order
            labeled_chunks.sort(key=lambda x: x.get("chunk_idx", 0))
            
            K = 4
            
            for i in range(len(labeled_chunks) - K - 1):  # Need at least 5 chunks to check
                current_chunk = labeled_chunks[i]
                # Check if the current and next 4 chunks have consistent high/low accuracy
                if "accuracy" in current_chunk:
                    # Check for high accuracy convergence (> 0.99)
                    high_convergence = all(
                        "accuracy" in labeled_chunks[i+j] and labeled_chunks[i+j]["accuracy"] > 0.99
                        for j in range(K)
                    )
                    # Check for low accuracy convergence (< 0.01)
                    low_convergence = all(
                        "accuracy" in labeled_chunks[i+j] and labeled_chunks[i+j]["accuracy"] < 0.01
                        for j in range(K)
                    )
                    
                    if high_convergence or low_convergence:
                        converged_from_index = i
                        break
            
            # Process each chunk
            for chunk in labeled_chunks:
                chunk_idx = chunk.get("chunk_idx")
                
                # Skip if no chunk index
                if chunk_idx is None:
                    print(f"Warning: {chunks_file} has no chunk index, skipping")
                    continue
                
                # Skip chunks that are past the convergence point
                if converged_from_index is not None and chunk_idx >= converged_from_index:
                    filtered_convergence += 1
                    continue
                
                # Calculate normalized position (0-1)
                normalized_position = chunk_idx / (total_chunks - 1) if total_chunks > 1 else 0.5
                
                # Get function tags
                function_tags = chunk.get("function_tags", [])
                
                # Skip if no function tags
                if not function_tags:
                    print(f"Warning: {chunks_file} has no function tags, skipping")
                    continue
                
                # Use the first function tag
                primary_tag = function_tags[0]
                
                # Skip unknown tags
                if primary_tag.lower() == "unknown":
                    print(f"Warning: {chunks_file} has unknown tag, skipping")
                    continue
                
                # Create a record for this chunk
                record = {
                    "problem_idx": problem_number,
                    "pn_ci": pn_ci,
                    "chunk_idx": chunk_idx,
                    "normalized_position": normalized_position,
                    "tag": primary_tag,
                    "is_correct": is_correct
                }
                
                # Store different_trajectories_fraction for filtering later
                different_trajectories_fraction = chunk.get("different_trajectories_fraction", 0.0)
                overdeterminedness = chunk.get("overdeterminedness", 0.0)
                record["different_trajectories_fraction"] = different_trajectories_fraction
                record["overdeterminedness"] = overdeterminedness
                
                # Add all importance metrics
                all_metrics_present = all(metric in chunk for metric in IMPORTANCE_METRICS)
                if all_metrics_present:
                    for metric in IMPORTANCE_METRICS:                        
                        record[metric] = abs(chunk.get(metric)) if chunk.get(metric) > 0.0 else 0.0
                
                tag_data.append(record)
                total_chunks_with_tags += 1
    
    # Convert to DataFrame
    df_tags = pd.DataFrame(tag_data)
    
    # Check if we have any data
    if df_tags.empty:
        print("No tag data found")
        return None
    
    # Print filtering statistics
    print(f"Total chunks with valid tags processed: {total_chunks_with_tags}")
    print(f"Chunks filtered out due to convergence: {filtered_convergence}")
    print(f"Chunks filtered out due to zero different trajectories for counterfactual_importance_accuracy: {filtered_counterfactual_accuracy}")
    print(f"Chunks filtered out due to zero different trajectories for counterfactual_importance_kl: {filtered_counterfactual_kl}")
    
    # Print counts of counterfactual metrics after filtering
    if "counterfactual_importance_accuracy" in df_tags.columns:
        print(f"Count of chunks with counterfactual_importance_accuracy after filtering: {df_tags['counterfactual_importance_accuracy'].notna().sum()}")
    if "counterfactual_importance_kl" in df_tags.columns:
        print(f"Count of chunks with counterfactual_importance_kl after filtering: {df_tags['counterfactual_importance_kl'].notna().sum()}")
    
    print(f"Collected data for {len(df_tags)} chunks with tags")
    
    return df_tags

def aggregate_tag_data(df_tags):
    """
    Aggregate tag data to get mean, std, and count for each tag.
    Uses proper hierarchical statistics if args.hierarchical is True: first computes problem-level averages,
    then computes grand averages and standard errors across problem averages.
    Otherwise uses original chunk-level statistics.
    
    Args:
        df_tags: DataFrame with tag data
        
    Returns:
        DataFrame with aggregated tag data
    """
    # Collect aggregated data
    aggregated_data = []
    
    # Group by tag
    tag_groups = df_tags.groupby("tag")
    
    for tag, group in tag_groups:
        # Skip if too few data points
        if len(group) < 3:
            print(f"Skipping tag {tag} because it has less than 3 data points")
            continue
        
        if args.hierarchical:
            # Use hierarchical approach: problem-level averages first
            
            # Step 1: Calculate problem-level averages for position
            problem_position_averages = []
            position_problem_groups = group.groupby("pn_ci")
            
            for pn_ci, problem_group in position_problem_groups:
                if len(problem_group) > 0:
                    problem_avg_position = problem_group["normalized_position"].mean()
                    problem_position_averages.append(problem_avg_position)
            
            # Calculate grand average and standard error for position across problems
            if len(problem_position_averages) == 0:
                print(f"Skipping tag {tag} because no problem-level position averages could be computed")
                continue
                
            pos_mean = np.mean(problem_position_averages)
            pos_std = np.std(problem_position_averages, ddof=1) / np.sqrt(len(problem_position_averages)) if len(problem_position_averages) > 1 else 0.0
            num_problems = len(problem_position_averages)
            
            # Step 2: Calculate problem-level averages for each importance metric
            for metric in IMPORTANCE_METRICS:
                if metric not in group.columns:
                    print(f"Skipping metric {metric} for tag {tag} because it is not in the dataframe")
                    continue
                
                # Calculate problem-level averages for this metric
                problem_metric_averages = []
                metric_problem_groups = group.dropna(subset=[metric]).groupby("pn_ci")
                
                for pn_ci, problem_group in metric_problem_groups:
                    if len(problem_group) > 0:
                        problem_avg_metric = problem_group[metric].mean()
                        problem_metric_averages.append(problem_avg_metric)
                
                # Skip if not enough problem-level averages
                if len(problem_metric_averages) < 3:
                    print(f"Skipping metric {metric} for tag {tag} because it has less than 3 problem-level averages")
                    continue
                
                # Calculate grand average and standard error across problem averages
                metric_mean = np.mean(problem_metric_averages)
                metric_std = np.std(problem_metric_averages, ddof=1) / np.sqrt(len(problem_metric_averages)) if len(problem_metric_averages) > 1 else 0.0
                num_problems_for_metric = len(problem_metric_averages)
                
                # Add to aggregated data
                aggregated_data.append({
                    "tag": tag,
                    "metric": metric,
                    "position_mean": pos_mean,
                    "position_std": pos_std,
                    "metric_mean": metric_mean,
                    "metric_std": metric_std,
                    "count": num_problems_for_metric,  # Number of problems that contributed to this metric
                    "total_chunks": len(group)  # Total number of chunks for reference
                })
        
        else:
            # Use original chunk-level approach
            
            # Calculate position statistics
            pos_mean = group["normalized_position"].mean()
            # Use standard error instead of std deviation for position
            pos_std = group["normalized_position"].std() / np.sqrt(len(group))
            count = len(group)
            
            # Calculate statistics for each importance metric
            for metric in IMPORTANCE_METRICS:
                if metric not in group.columns:
                    print(f"Skipping metric {metric} for tag {tag} because it is not in the dataframe")
                    continue
                    
                # Skip if not enough valid values
                valid_values = group[metric].dropna()
                if len(valid_values) < 3:
                    print(f"Skipping metric {metric} for tag {tag} because it has less than 3 data points")
                    continue
                
                # Calculate statistics
                metric_mean = valid_values.mean()
                metric_std = valid_values.std() / np.sqrt(len(valid_values))  # Standard error
                
                # Add to aggregated data
                aggregated_data.append({
                    "tag": tag,
                    "metric": metric,
                    "position_mean": pos_mean,
                    "position_std": pos_std,
                    "metric_mean": metric_mean,
                    "metric_std": metric_std,
                    "count": count
                })
    
    # Convert to DataFrame
    df_agg = pd.DataFrame(aggregated_data)
    
    return df_agg

def plot_tag_effects(df_agg, output_dir="analysis/importance_plots"):
    """
    Create plots showing tag effects vs. position for each importance metric.
    
    Args:
        df_agg: DataFrame with aggregated tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Filter out unwanted categories
    df_agg = df_agg[~df_agg['tag'].apply(lambda x: ' '.join(word.capitalize() for word in x.split('_'))).isin(categories_to_exclude)]
    
    # Create a plot for each importance metric
    for metric in IMPORTANCE_METRICS:
        # Filter data for this metric
        df_metric = df_agg[df_agg["metric"] == metric]
        
        # Skip if no data
        if df_metric.empty:
            print(f"No data for metric {metric}")
            continue
        
        # Create plot
        plt.figure(figsize=FIGSIZE)
        
        # Get max count for normalizing dot sizes
        max_count = df_metric["count"].max()
        min_count = df_metric["count"].min()
        
        # Format tag names nicely by replacing underscores with spaces and capitalizing words
        df_metric['formatted_tag'] = df_metric['tag'].apply(
            lambda x: ' '.join(word.capitalize() for word in x.split('_'))
        )
        
        # Determine which name use for the plot
        if "resampling" in metric:
            plot_type = "Resampling"
        elif "counterfactual" in metric:
            plot_type = "Counterfactual"
        elif "forced" in metric:
            plot_type = "Forced"
        else:
            plot_type = ""
            
        if "accuracy" in metric:
            measure = "Accuracy"
        elif "kl" in metric:
            measure = "KL divergence"
        else:
            measure = ""
            
        # Use our consistent color palette
        all_categories = df_metric['formatted_tag'].unique()
        
        # Create a mapping for each category using our predefined colors
        category_colors = {}
        for category in all_categories:
            # Use the predefined color if available, otherwise use a default color
            category_colors[category] = CATEGORY_COLORS.get(category, '#7f7f7f')  # Default to gray
        
        # Plot each tag with a unique color and legend entry
        for idx, row in df_metric.iterrows():
            # Calculate dot size based on count
            size = 100 + 900 * (row["count"] - min_count) / (max_count - min_count) if max_count > min_count else 500
            
            # Get proper color for this tag
            tag_color = category_colors.get(row["formatted_tag"])
            
            # Plot point with error bars
            plt.errorbar(
                row["position_mean"], 
                row["metric_mean"], 
                xerr=row["position_std"],
                yerr=row["metric_std"],
                fmt='o',
                markersize=np.sqrt(size/np.pi),  # Convert area to radius
                alpha=0.7,
                capsize=5,
                label=row["formatted_tag"],
                color=tag_color
            )
        
        # Add grid
        # plt.grid(alpha=0.3, linestyle='--')
        
        # Set x-axis range to full 0-1 range
        plt.xlim(-0.05, 1.05)
        
        # Calculate y-axis range to zoom in (use metric means +/- std)
        y_values = df_metric["metric_mean"]
        y_errors = df_metric["metric_std"]
        y_min = min(y_values - y_errors)
        y_max = max(y_values + y_errors)
        y_range = y_max - y_min
        plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)
        plt.xlim(0.185, 0.615)
        
        # Set labels and title
        plt.xlabel("Normalized position in trace (0-1)")
        plt.ylabel(f"{plot_type} importance")
        plt.title(f"Sentence category effect" if args.model == 'qwen-14b' else f"Sentence category effect\n(R1-Distill-Llama-8B)")
        
        # Add legend in bottom center with multiple columns if needed
        num_entries = len(df_metric)
        if num_entries > 10:
            ncol = 2
        else:
            ncol = 1
        
        # Create legend with smaller marker sizes to avoid overlapping
        handles, labels = plt.gca().get_legend_handles_labels()
        
        # Make the markers in the legend smaller than in the plot
        for handle in handles:
            if hasattr(handle, 'set_markersize'):
                handle.set_markersize(6)  # Smaller marker size in legend
        
        """"
        # Add padding and adjust layout for the legend
        legend = plt.legend(
            handles, 
            labels,
            loc='lower center',  # Changed to bottom center
            bbox_to_anchor=(0.5, -0.25),  # Position below the plot
            ncol=ncol, 
            framealpha=0.7, 
            # fontsize=10,
            borderpad=1.0,  # Padding between legend border and content
            labelspacing=1.0,  # Vertical space between legend entries
            handletextpad=0.5,  # Space between marker and text
        )
        """
        
        # Add some extra padding to avoid plot edge
        plt.subplots_adjust(bottom=0.25)  # Make room for the legend at the bottom
        
        # Add tight layout
        plt.tight_layout()
        
        # Save plot
        plt.savefig(os.path.join(output_dir, f"{metric}_tag_effects.png"), dpi=300)
        plt.close()
        
        print(f"Created plot for {metric}")
    
    print(f"All plots saved to {output_dir}")

def plot_debiased_tag_effects(df_tags, output_dir="analysis/importance_plots"):
    """
    Create position-debiased plots showing tag effects for each importance metric.
    Uses a residualization approach to remove position bias from importance metrics.
    
    Args:
        df_tags: DataFrame with raw tag data (not aggregated)
        output_dir: Directory to save plots
    """
    if not HAS_SKLEARN:
        print("Skipping position-debiased plots because scikit-learn is not installed.")
        return
        
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Process each importance metric
    for metric in IMPORTANCE_METRICS:
        # Skip if metric not in dataframe
        if metric not in df_tags.columns:
            print(f"Metric {metric} not found in data, skipping debiased plot")
            continue
        
        # Create a copy of the dataframe with only rows that have this metric
        df_metric = df_tags[df_tags[metric].notna()].copy()
        
        # Skip if no data
        if df_metric.empty:
            print(f"No data for metric {metric}, skipping debiased plot")
            continue
        
        print(f"Creating position-debiased plot for {metric}...")
        
        # Step 1: Fit a non-linear regression model to predict importance from position
        # We'll use a polynomial regression of degree 10 to capture non-linear effects
        
        # Create the position features
        X = df_metric[['normalized_position']].values
        y = df_metric[metric].values
        
        # Create and fit the model
        model = make_pipeline(PolynomialFeatures(degree=10), LinearRegression())
        model.fit(X, y)
        
        # Calculate predicted values based on position
        predicted_importance = model.predict(X)
        
        # Calculate residuals (observed - predicted)
        df_metric['debiased_importance'] = df_metric[metric] - predicted_importance
        
        # Step 2: Aggregate debiased data by tag using conditional approach
        # Group by tag and calculate statistics
        debiased_data = []
        
        # Format tag names for nicer display
        df_metric['formatted_tag'] = df_metric['tag'].apply(
            lambda x: ' '.join(word.capitalize() for word in x.split('_'))
        )
        
        for tag, group in df_metric.groupby('tag'):
            # Skip if too few data points
            if len(group) < 3:
                print(f"Skipping tag {tag} in debiased plot because it has less than 3 data points")
                continue
            
            if args.hierarchical:
                # Use hierarchical approach: problem-level averages first
                
                # Calculate problem-level averages for position
                problem_position_averages = []
                position_problem_groups = group.groupby("pn_ci")
                
                for pn_ci, problem_group in position_problem_groups:
                    if len(problem_group) > 0:
                        problem_avg_position = problem_group["normalized_position"].mean()
                        problem_position_averages.append(problem_avg_position)
                
                # Calculate problem-level averages for debiased importance
                problem_debiased_averages = []
                debiased_problem_groups = group.groupby("pn_ci")
                
                for pn_ci, problem_group in debiased_problem_groups:
                    if len(problem_group) > 0:
                        problem_avg_debiased = problem_group["debiased_importance"].mean()
                        problem_debiased_averages.append(problem_avg_debiased)
                
                # Skip if not enough problem-level averages
                if len(problem_position_averages) < 3 or len(problem_debiased_averages) < 3:
                    print(f"Skipping tag {tag} in debiased plot because it has less than 3 problem-level averages")
                    continue
                
                # Calculate grand averages and standard errors across problem averages
                pos_mean = np.mean(problem_position_averages)
                pos_std = np.std(problem_position_averages, ddof=1) / np.sqrt(len(problem_position_averages)) if len(problem_position_averages) > 1 else 0.0
                
                debiased_mean = np.mean(problem_debiased_averages)
                debiased_std = np.std(problem_debiased_averages, ddof=1) / np.sqrt(len(problem_debiased_averages)) if len(problem_debiased_averages) > 1 else 0.0
                
                count = len(problem_debiased_averages)  # Number of problems
            
            else:
                # Use original chunk-level approach
                
                # Calculate position statistics
                pos_mean = group["normalized_position"].mean()
                # Use standard error instead of std deviation for position
                pos_std = group["normalized_position"].std() / np.sqrt(len(group))
                count = len(group)
                
                # Calculate debiased importance statistics
                debiased_mean = group["debiased_importance"].mean()
                debiased_std = group["debiased_importance"].std() / np.sqrt(count)  # Standard error
            
            # Get the formatted tag name
            formatted_tag = group['formatted_tag'].iloc[0]
            
            debiased_data.append({
                "tag": tag,
                "formatted_tag": formatted_tag,
                "position_mean": pos_mean,
                "position_std": pos_std,
                "debiased_mean": debiased_mean,
                "debiased_std": debiased_std,
                "count": count
            })
        
        # Skip if no aggregated data
        if not debiased_data:
            print(f"No aggregated data for metric {metric}, skipping debiased plot")
            continue
        
        # Convert to DataFrame
        df_debiased = pd.DataFrame(debiased_data)
        
        # Step 3: Create the debiased plot
        plt.figure(figsize=FIGSIZE)
        
        # Get max count for normalizing dot sizes
        max_count = df_debiased["count"].max()
        min_count = df_debiased["count"].min()
        
        # Determine which name to use for the plot
        if "resampling" in metric:
            plot_type = "Resampling"
        elif "counterfactual" in metric:
            plot_type = "Counterfactual"
        elif "forced" in metric:
            plot_type = "Forced"
        else:
            plot_type = ""
            
        if "accuracy" in metric:
            measure = "Accuracy"
        elif "kl" in metric:
            measure = "KL divergence"
        else:
            measure = ""
            
        # Use our consistent color palette
        all_categories = df_debiased['formatted_tag'].unique()
        
        # Create a mapping for each category using our predefined colors
        category_colors = {}
        for category in all_categories:
            # Use the predefined color if available, otherwise use a default color
            category_colors[category] = CATEGORY_COLORS.get(category, '#7f7f7f')  # Default to gray
        
        # Plot each tag
        for _, row in df_debiased.iterrows():
            # Calculate dot size based on count
            size = 100 + 900 * (row["count"] - min_count) / (max_count - min_count) if max_count > min_count else 500
            
            # Get proper color for this tag
            tag_color = category_colors.get(row["formatted_tag"])
            
            # Plot point with error bars
            plt.errorbar(
                row["position_mean"], 
                row["debiased_mean"], 
                xerr=row["position_std"],
                yerr=row["debiased_std"],
                fmt='o',
                markersize=np.sqrt(size/np.pi),  # Convert area to radius
                alpha=0.7,
                capsize=5,
                label=row["formatted_tag"],
                color=tag_color
            )
            
        # Add best fit line for debiased data
        if len(df_debiased) >= 5:
            # Get position and debiased values
            x_values = df_debiased["position_mean"].values.reshape(-1, 1)
            y_values = df_debiased["debiased_mean"].values
            
            # Fit polynomial regression model (degree 2 for a curved line)
            model = make_pipeline(PolynomialFeatures(degree=1), LinearRegression())
            model.fit(x_values, y_values)
            
            # Generate prediction line points
            x_range = np.linspace(0, 1, 100).reshape(-1, 1)
            y_pred = model.predict(x_range)
            
            # Calculate correlation coefficient
            correlation = np.corrcoef(df_debiased["position_mean"], df_debiased["debiased_mean"])[0, 1]
            
            # Plot best fit line
            plt.plot(x_range, y_pred, color='gray', linestyle='--', linewidth=2, label=f'Best Fit Line (r = {correlation:.2f})')
        
        # Add grid
        # plt.grid(alpha=0.3, linestyle='--')
        
        # Set axis limits for x
        plt.xlim(-0.05, 1.05)
        
        # Set labels and title
        plt.xlabel("Normalized position in trace (0-1)")
        
        if "accuracy" in metric:
            plt.ylabel(f"Position-Debiased {plot_type} Importance\n({measure})")
            if args.hierarchical:
                plt.title(f"Position-Debiased Sentence Category Effect vs. Position (Problem-Level Averages)")
            else:
                plt.title(f"Position-Debiased Sentence Category Effect vs. Position")
        elif "kl" in metric:
            plt.ylabel(f"Position-Debiased {plot_type} Importance\n({measure})")
            if args.hierarchical:
                plt.title(f"Position-Debiased Sentence Category Effect vs. Position (Problem-Level Averages)")
            else:
                plt.title(f"Position-Debiased Sentence Category Effect vs. Position")
        
        # Add legend in bottom center with multiple columns if needed
        num_entries = len(df_debiased) + 1  # +1 for best fit line
        if num_entries > 10:
            ncol = 2
        else:
            ncol = 1
        
        # Create legend with smaller marker sizes to avoid overlapping
        handles, labels = plt.gca().get_legend_handles_labels()
        
        # Move the best fit line to the end of the legend
        if 'Best Fit Line' in ''.join(labels):
            for i, label in enumerate(labels):
                if 'Best Fit Line' in label:
                    # Move this item to the end
                    best_fit_handle = handles.pop(i)
                    best_fit_label = labels.pop(i)
                    handles.append(best_fit_handle)
                    labels.append(best_fit_label)
                    break
        
        # Make the markers in the legend smaller than in the plot
        for handle in handles:
            if hasattr(handle, 'set_markersize'):
                handle.set_markersize(6)  # Smaller marker size in legend
        
        # Add padding and adjust layout for the legend
        legend = plt.legend(
            handles, 
            labels,
            loc='lower center', 
            bbox_to_anchor=(0.5, -0.25),  # Position below the plot
            ncol=ncol, 
            framealpha=0.7, 
            # fontsize=10,
            borderpad=1.0,  # Padding between legend border and content
            labelspacing=1.0,  # Vertical space between legend entries
            handletextpad=0.5  # Space between marker and text
        )
        
        # Add some extra padding to avoid plot edge
        plt.subplots_adjust(bottom=0.25)  # Make room for the legend at the bottom
        
        # Add tight layout
        plt.tight_layout()
        
        # Save plot
        plt.savefig(os.path.join(output_dir, f"{metric}_tag_effects_no_bias.png"), dpi=300)
        plt.close()
        
        # Save the debiased data for further analysis
        df_debiased.to_csv(os.path.join(output_dir, f"{metric}_debiased_data.csv"), index=False)
        
        print(f"Created position-debiased plot for {metric}")
        
        # Step 4: Additional visualization: Raw importance vs. position with trendline
        plt.figure(figsize=FIGSIZE)
        
        # Create scatter plot of all data points
        plt.scatter(df_metric['normalized_position'], df_metric[metric], alpha=0.3, s=10)
        
        # Add the trend line
        positions = np.linspace(0, 1, 100).reshape(-1, 1)
        predictions = model.predict(positions)
        plt.plot(positions, predictions, 'r-', linewidth=2, label='Position Trend')
        
        # Add labels
        plt.xlabel("Normalized position in trace (0-1)")
        
        if "accuracy" in metric:
            plt.ylabel(f"{plot_type} Importance\n({measure})")
            plt.title(f"Position Bias in {plot_type} Importance\n({measure})")
        elif "kl" in metric:
            plt.ylabel(f"{plot_type} Importance\n({measure})")
            plt.title(f"Position Bias in {plot_type} Importance\n({measure})")
        
        plt.legend()
        # plt.grid(alpha=0.3, linestyle='--')
        plt.tight_layout()
        
        # Save the trend visualization
        plt.savefig(os.path.join(output_dir, f"{metric}_position_trend.png"), dpi=300)
        plt.close()

def plot_kl_metric_comparison(df_tags, output_dir="analysis/importance_plots"):
    """
    Create scatter plots comparing KL-based metrics against each other.
    
    Args:
        df_tags: DataFrame with raw tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Identify KL-based metrics
    kl_metrics = [m for m in IMPORTANCE_METRICS if 'kl' in m]
    
    # Create a copy of the dataframe
    df = df_tags.copy()
    
    # Format tag names for nicer display
    df['formatted_tag'] = df['tag'].apply(
        lambda x: ' '.join(word.capitalize() for word in x.split('_'))
    )
    
    # Filter out unwanted categories
    df = df[~df['formatted_tag'].isin(categories_to_exclude)]
    
    # Use our consistent color palette
    all_categories = sorted(df['formatted_tag'].unique())
    
    # Create a mapping for each category using our predefined colors
    category_colors = {}
    for category in all_categories:
        # Use the predefined color if available, otherwise use a default color
        category_colors[category] = CATEGORY_COLORS.get(category, '#7f7f7f')  # Default to gray
    
    # Generate all pairwise combinations of KL metrics
    metric_pairs = [(kl_metrics[i], kl_metrics[j]) 
                    for i in range(len(kl_metrics)) 
                    for j in range(i+1, len(kl_metrics))]
    
    print(f"Generating {len(metric_pairs)} KL metric comparison plots...")
    
    # Plot each pair of metrics
    for x_metric, y_metric in metric_pairs:
        # Filter data where both metrics are available
        try:
            df_pair = df.dropna(subset=[x_metric, y_metric])
        except:
            print(f"Skipping comparison of {x_metric} vs {y_metric} (ERROR)")
            continue
        
        # Skip if not enough data
        if len(df_pair) < 3:
            print(f"Skipping comparison of {x_metric} vs {y_metric} (not enough data)")
            continue
            
        print(f"Creating scatter plot for {x_metric} vs {y_metric}...")
        
        # Get unique categories for subplot creation
        categories = sorted(df_pair['formatted_tag'].unique())
        
        # Create 2x2 subplots
        fig, axes = plt.subplots(2, 2, figsize=FIGSIZE)  # 2x(3.5, 3.5)
        fig.suptitle("Comparing counterfactual vs. forced", fontsize=12)
        
        # Set default font size
        # plt.rcParams.update({'font.size': 11})
        
        # Plot each category in its own subplot
        for i, category in enumerate(categories):
            if i >= 4:  # Only use first 4 categories for 2x2 grid
                break
                
            row = i // 2
            col = i % 2
            ax = axes[row, col]
            
            # Get data for this category
            cat_data = df_pair[df_pair['formatted_tag'] == category]
            
            # Skip if too few points
            if len(cat_data) < 3:
                ax.text(0.5, 0.5, f"Insufficient data for\n{category}", 
                        horizontalalignment='center', verticalalignment='center',
                        transform=ax.transAxes)
                ax.set_xticks([])
                ax.set_yticks([])
                continue
            
            # Scatter plot for this category
            ax.scatter(cat_data[y_metric], cat_data[x_metric], s=40, alpha=0.7, color=category_colors[category])
            
            # Set title
            ax.set_title(category, fontsize=11)
            # ax.grid(True, alpha=0.3, linestyle='--')
            
            # Set axis limits
            ax.set_xlim(-0.05, 5.05)  # Forced importance (x-axis)
            ax.set_ylim(-0.05, 0.55)   # Counterfactual importance (y-axis)
            
            # Add axis labels
            ax.set_xlabel("Forced importance", fontsize=11)
            ax.set_ylabel("Counterfactual importance", fontsize=11)
            
            # Remove top and right spines
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            
            # Add legend with no frame
            if i == 0:  # Only add legend to first subplot
                ax.legend(frameon=False)
        
        # Hide any unused subplots
        for i in range(len(categories), 4):
            row = i // 2
            col = i % 2
            axes[row, col].set_visible(False)
        
        # Adjust spacing
        plt.tight_layout()
        plt.subplots_adjust(top=0.92)  # Make room for suptitle
        
        # Save subplots
        subplot_filename = f"{x_metric}_vs_{y_metric}_category_subplots.png"
        plt.savefig(os.path.join(output_dir, subplot_filename), dpi=300)
        plt.close()
        
        print(f"Saved category subplots to {subplot_filename}")
    
    print("KL metric comparison plots complete.")

def plot_overdeterminedness_comparison(df_tags, output_dir="analysis/importance_plots"):
    """
    Create scatter plots comparing importance metrics against overdeterminedness.
    Overdeterminedness is measured as 1 - different_trajectories_fraction.
    Importance values are normalized per problem to handle scale differences if args.normalize is True.
    
    Args:
        df_tags: DataFrame with raw tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Format tag names for nicer display
    df = df_tags.copy()
    df['formatted_tag'] = df['tag'].apply(
        lambda x: ' '.join(word.capitalize() for word in x.split('_'))
    )
    
    # Calculate overdeterminedness
    df['overdeterminedness'] = 1 - df['different_trajectories_fraction']
    df = df[df['overdeterminedness'] <= 0.95]
    
    # Use our consistent color palette
    all_categories = sorted(df['formatted_tag'].unique())
    category_colors = {cat: CATEGORY_COLORS.get(cat, '#7f7f7f') for cat in all_categories}
    
    # Define metrics to plot with their display names
    metrics_to_plot = {
        'counterfactual_importance_accuracy': 'Counterfactual importance (Accuracy)',
        'counterfactual_importance_kl': 'Counterfactual importance (KL)',
        'resampling_importance_accuracy': 'Resampling importance (Accuracy)',
        'resampling_importance_kl': 'Resampling importance (KL)',
        'forced_importance_accuracy': 'Forced importance (Accuracy)',
        'forced_importance_kl': 'Forced importance (KL)'
    }
    
    norm_status = "normalized" if args.normalize else "raw"
    print(f"Generating {norm_status} overdeterminedness comparison plots...")
    
    for metric, display_name in metrics_to_plot.items():
        # Skip if metric not in data
        if metric not in df.columns:
            print(f"Skipping {metric} - not found in data")
            continue
        
        # Normalize importance values per problem if requested
        if args.normalize:
            df[f'{metric}_normalized'] = df.groupby('pn_ci')[metric].transform(
                lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0.5
            )
            plot_metric = f'{metric}_normalized'
            y_label_prefix = 'Normalized '
            title_suffix = '\n(Importance Normalized per Problem)'
        else:
            plot_metric = metric
            y_label_prefix = ''
            title_suffix = '\n(Raw Importance Values)'
        
        # Create figure
        plt.figure(figsize=FIGSIZE)
        
        # Create scatter plot for each category
        for category in all_categories:
            cat_data = df[df['formatted_tag'] == category]
            
            # Skip if no valid data for this category
            if len(cat_data) < 3:
                continue
            
            plt.scatter(
                cat_data['overdeterminedness'],
                cat_data[plot_metric],
                alpha=0.7,
                s=50,
                label=category,
                color=category_colors[category],
                edgecolors='white',
                linewidths=0.5
            )
        
        # Add best fit line if sklearn available
        if HAS_SKLEARN:
            # Get valid data points
            valid_data = df.dropna(subset=['overdeterminedness', plot_metric])
            
            if len(valid_data) >= 5:
                x_values = valid_data['overdeterminedness'].values.reshape(-1, 1)
                y_values = valid_data[plot_metric].values
                
                # Fit linear regression
                model = LinearRegression()
                model.fit(x_values, y_values)
                
                # Generate prediction line points
                x_range = np.linspace(0, 1, 100).reshape(-1, 1)
                y_pred = model.predict(x_range)
                
                # Clip predicted values to [0,1] range if using normalized values
                if args.normalize:
                    y_pred = np.clip(y_pred, 0, 1)
                
                # Calculate correlation coefficient
                correlation = np.corrcoef(valid_data['overdeterminedness'], valid_data[plot_metric])[0, 1]
                
                # Plot regression line
                plt.plot(x_range, y_pred, color='red', linestyle='--', linewidth=2,
                        label=f'Best Fit (rÂ²={model.score(x_values, y_values):.3f})')
                
                # Add correlation coefficient
                plt.annotate(f'Correlation: {correlation:.3f}',
                           xy=(0.05, 0.95),
                           xycoords='axes fraction',
                           fontsize=12,
                           bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.7))
        
        # Add grid
        # plt.grid(True, alpha=0.3, linestyle='--')
        
        # Set axis labels and title
        plt.xlabel('Overdeterminedness (1 - Different Trajectories Fraction)')
        plt.ylabel(f'{y_label_prefix}{display_name}')
        plt.title(f'{display_name} vs. Overdeterminedness{title_suffix}')
        
        # Set axis limits
        plt.xlim(-0.05, 1.05)
        if args.normalize:
            plt.ylim(-0.05, 1.05)
        
        # Create legend with smaller marker sizes
        handles, labels = plt.gca().get_legend_handles_labels()
        for handle in handles:
            if hasattr(handle, 'set_markersize'):
                handle.set_markersize(6)
        
        # Move legend outside the plot
        if len(all_categories) > 10:
            plt.legend(
                handles,
                labels,
                bbox_to_anchor=(1.05, 1),
                loc='upper left',
                ncol=2,
                framealpha=0.7,
                borderpad=1.0,
                labelspacing=1.0,
                handletextpad=0.5
            )
        else:
            plt.legend(
                handles,
                labels,
                bbox_to_anchor=(1.05, 1),
                loc='upper left',
                ncol=1,
                framealpha=0.7,
                borderpad=1.0,
                labelspacing=1.0,
                handletextpad=0.5
            )
        
        # Adjust layout
        plt.subplots_adjust(right=0.75)  # Make room for the legend
        plt.tight_layout()
        
        # Save plot
        norm_indicator = "normalized" if args.normalize else "raw"
        metric_name = metric.replace('_importance_', '_vs_overdeterminedness_')
        plot_filename = f"{metric_name}_{norm_indicator}.png"
        plt.savefig(os.path.join(output_dir, plot_filename), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Saved plot to {plot_filename}")
    
    print(f"{norm_status.capitalize()} overdeterminedness comparison plots complete.")

def plot_overdeterminedness_category_subplots(df_tags, output_dir="analysis/importance_plots"):
    """
    Create subplot-based visualizations comparing importance vs. overdeterminedness for each category.
    Creates separate files for each metric type (counterfactual, resampling, forced).
    Importance values are normalized per problem if args.normalize is True, otherwise raw values are used.
    
    Args:
        df_tags: DataFrame with raw tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Format tag names for nicer display
    df = df_tags.copy()
    df['formatted_tag'] = df['tag'].apply(
        lambda x: ' '.join(word.capitalize() for word in x.split('_'))
    )
    
    # Calculate overdeterminedness
    df['overdeterminedness'] = 1 - df['different_trajectories_fraction']
    
    # Define metrics to plot with their display names and pairs
    metric_groups = {
        'counterfactual': {
            'accuracy': ('counterfactual_importance_accuracy', 'Counterfactual importance (Accuracy)'),
            'kl': ('counterfactual_importance_kl', 'Counterfactual importance (KL)')
        },
        'resampling': {
            'accuracy': ('resampling_importance_accuracy', 'Resampling importance (Accuracy)'),
            'kl': ('resampling_importance_kl', 'Resampling importance (KL)')
        },
        'forced': {
            'accuracy': ('forced_importance_accuracy', 'Forced importance (Accuracy)'),
            'kl': ('forced_importance_kl', 'Forced importance (KL)')
        }
    }
    
    # Get unique categories
    categories = sorted(df['formatted_tag'].unique())
    
    # Calculate grid dimensions
    n_categories = len(categories)
    n_cols = min(3, n_categories)
    n_rows = (n_categories + n_cols - 1) // n_cols
    
    norm_status = "normalized" if args.normalize else "raw"
    print(f"Generating {norm_status} overdeterminedness category subplot comparisons...")
    
    # Create plots for each metric group
    for group_name, metrics in metric_groups.items():
        # Skip if neither metric exists in data
        if not any(metric in df.columns for metric, _ in metrics.values()):
            print(f"Skipping {group_name} metrics - not found in data")
            continue
            
        # Normalize metrics per problem if requested
        for metric_type, (metric_name, _) in metrics.items():
            if metric_name in df.columns and args.normalize:
                df[f'{metric_name}_normalized'] = df.groupby('pn_ci')[metric_name].transform(
                    lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0.5
                )
        
        # Create figure
        fig = plt.figure(figsize=(6*n_cols, 5*n_rows))
        
        # Create subplots for each category
        for idx, category in enumerate(categories, 1):
            ax = fig.add_subplot(n_rows, n_cols, idx)
            
            # Filter data for this category
            cat_data = df[df['formatted_tag'] == category]
            
            # Plot both accuracy and KL metrics if available
            for metric_type, (metric_name, display_name) in metrics.items():
                if metric_name not in df.columns:
                    continue
                
                # Determine which metric to plot
                if args.normalize:
                    plot_metric = f'{metric_name}_normalized'
                else:
                    plot_metric = metric_name
                
                # Create scatter plot
                color = 'blue' if metric_type == 'accuracy' else 'red'
                label = 'Accuracy' if metric_type == 'accuracy' else 'KL'
                
                ax.scatter(
                    cat_data['overdeterminedness'],
                    cat_data[plot_metric],
                    alpha=0.7,
                    s=30,
                    label=label,
                    color=color
                )
                
                # Add best fit line if sklearn available and enough points
                if HAS_SKLEARN:
                    valid_data = cat_data.dropna(subset=['overdeterminedness', plot_metric])
                    if len(valid_data) >= 5:
                        x_values = valid_data['overdeterminedness'].values.reshape(-1, 1)
                        y_values = valid_data[plot_metric].values
                        
                        model = LinearRegression()
                        model.fit(x_values, y_values)
                        
                        x_range = np.linspace(0, 1, 100).reshape(-1, 1)
                        y_pred = model.predict(x_range)
                        
                        # Clip predicted values to [0,1] range when using normalized values
                        if args.normalize:
                            y_pred = np.clip(y_pred, 0, 1)
                        
                        correlation = np.corrcoef(valid_data['overdeterminedness'], valid_data[plot_metric])[0, 1]
                        
                        ax.plot(x_range, y_pred, color=color, linestyle='--', alpha=0.5,
                               label=f'{label} (r={correlation:.2f})')
            
            # Customize subplot
            ax.set_title(category)
            # ax.grid(True, alpha=0.3, linestyle='--')
            ax.set_xlabel('Overdeterminedness')
            
            if args.normalize:
                ax.set_ylabel('Normalized Importance')
                ax.set_ylim(-0.05, 1.05)
            else:
                ax.set_ylabel('Importance')
            
            # Set axis limits
            ax.set_xlim(-0.05, 1.05)
            
            # Add legend
            ax.legend(fontsize='small')
        
        # Adjust layout
        norm_suffix = " (Normalized per Problem)" if args.normalize else " (Raw Values)"
        plt.suptitle(f'{group_name.capitalize()} Importance vs. Overdeterminedness by Category{norm_suffix}', fontsize=16, y=1.02)
        plt.tight_layout()
        
        # Save plot
        norm_indicator = "normalized" if args.normalize else "raw"
        plot_filename = f"{group_name}_importance_vs_overdeterminedness_category_subplots_{norm_indicator}.png"
        plt.savefig(os.path.join(output_dir, plot_filename), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Saved {group_name} category subplots to {plot_filename}")
    
    print(f"{norm_status.capitalize()} category subplot comparisons complete.")

def plot_individual_points_by_category(df_tags, output_dir="analysis/importance_plots"):
    """
    Create scatter plots showing individual points colored by their categories for each importance metric.
    
    Args:
        df_tags: DataFrame with raw tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a copy of the dataframe
    df = df_tags.copy()
    
    # Format tag names for nicer display
    df['formatted_tag'] = df['tag'].apply(
        lambda x: ' '.join(word.capitalize() for word in x.split('_'))
    )
    
    # Filter out unwanted categories
    df = df[~df['formatted_tag'].isin(categories_to_exclude)]
    
    # Use our consistent color palette
    all_categories = sorted(df['formatted_tag'].unique())
    category_colors = {cat: CATEGORY_COLORS.get(cat, '#7f7f7f') for cat in all_categories}
    
    # Define metrics to plot with their display names
    metrics_to_plot = {
        'counterfactual_importance_kl': 'Counterfactual importance (KL)',
        'resampling_importance_kl': 'Resampling importance (KL)',
        'forced_importance_kl': 'Forced importance (KL)'
    }
    
    print("Generating individual points plots colored by category...")
    
    for metric, display_name in metrics_to_plot.items():
        # Skip if metric not in data
        if metric not in df.columns:
            print(f"Skipping {metric} - not found in data")
            continue
        
        # Create figure
        plt.figure(figsize=FIGSIZE)
        
        # Create scatter plot for each category
        for category in all_categories:
            cat_data = df[df['formatted_tag'] == category]
            
            # Skip if no valid data for this category
            if len(cat_data) < 3:
                continue
            
            plt.scatter(
                cat_data['normalized_position'],
                cat_data[metric],
                alpha=0.7,
                s=50,
                label=category,
                color=category_colors[category],
                edgecolors='white',
                linewidths=0.5
            )
        
        # Add best fit line if sklearn available
        if HAS_SKLEARN:
            # Get valid data points
            valid_data = df.dropna(subset=['normalized_position', metric])
            
            if len(valid_data) >= 5:
                x_values = valid_data['normalized_position'].values.reshape(-1, 1)
                y_values = valid_data[metric].values
                
                # Fit linear regression
                model = LinearRegression()
                model.fit(x_values, y_values)
                
                # Generate prediction line points
                x_range = np.linspace(0, 1, 100).reshape(-1, 1)
                y_pred = model.predict(x_range)
                
                # Calculate correlation coefficient
                correlation = np.corrcoef(valid_data['normalized_position'], valid_data[metric])[0, 1]
                
                # Plot regression line
                plt.plot(x_range, y_pred, color='red', linestyle='--', linewidth=2,
                        label=f'Best Fit (rÂ²={model.score(x_values, y_values):.3f})')
                
                # Add correlation coefficient
                plt.annotate(f'Correlation: {correlation:.3f}',
                           xy=(0.05, 0.95),
                           xycoords='axes fraction',
                           fontsize=12,
                           bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.7))
        
        # Add grid
        # plt.grid(False, alpha=0.3, linestyle='--')
        
        # Set axis labels and title
        plt.xlabel("Normalized position in trace (0-1)")
        plt.ylabel(display_name)
        plt.title(f'Individual Points by Category: {display_name}')
        
        # Set x-axis range to full 0-1 range
        plt.xlim(-0.05, 1.05)
        
        # Calculate y-axis range to zoom in (use 5th and 95th percentiles)
        valid_y = df[metric].dropna()
        y_min = np.percentile(valid_y, 5)
        y_max = np.percentile(valid_y, 95)
        y_range = y_max - y_min
        plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)
        
        # Create legend with smaller marker sizes
        handles, labels = plt.gca().get_legend_handles_labels()
        for handle in handles:
            if hasattr(handle, 'set_markersize'):
                handle.set_markersize(6)
        
        # Add legend with multiple columns if needed
        if len(all_categories) > 10:
            plt.legend(
                handles,
                labels,
                bbox_to_anchor=(1.05, 1),
                loc='upper left',
                ncol=2,
                framealpha=0.7,
                borderpad=1.0,
                labelspacing=1.0,
                handletextpad=0.5
            )
        else:
            plt.legend(
                handles,
                labels,
                bbox_to_anchor=(1.05, 1),
                loc='upper left',
                ncol=1,
                framealpha=0.7,
                borderpad=1.0,
                labelspacing=1.0,
                handletextpad=0.5
            )
        
        # Adjust layout to prevent legend cutoff
        plt.subplots_adjust(right=0.75)  # Make room for the legend
        plt.tight_layout()
        
        # Save plot
        plt.savefig(os.path.join(output_dir, f"{metric}_individual_points.png"), 
                   bbox_inches='tight', dpi=300)
        plt.close()
        
        print(f"Created individual points plot for {metric}")
    
    print(f"All individual points plots saved to {output_dir}")

def plot_category_metrics_averages(df_tags, output_dir="analysis/importance_plots"):
    """
    Create bar plots showing average overdeterminedness and different trajectories fraction by category.
    
    Args:
        df_tags: DataFrame with raw tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a copy of the dataframe
    df = df_tags.copy()
    
    # Format tag names for nicer display
    df['formatted_tag'] = df['tag'].apply(
        lambda x: ' '.join(word.capitalize() for word in x.split('_'))
    )
    
    # Filter out unwanted categories if needed
    # df = df[~df['formatted_tag'].isin(categories_to_exclude)]
    
    # Calculate metrics by category
    metrics = ['overdeterminedness', 'different_trajectories_fraction']
    
    # Define the category order from confusion matrix script
    confusion_matrix_order = [
        'Active Computation',
        'Fact Retrieval',
        'Plan Generation',
        'Uncertainty Management',
        'Result Consolidation',
        'Self Checking',
        'Problem Setup',
        'Final Answer Emission'
    ]
    
    for metric in metrics:
        # Skip if metric not in data
        if metric not in df.columns:
            print(f"Skipping {metric} - not found in data")
            continue
        
        # Group by category and calculate statistics
        grouped = df.groupby('formatted_tag')[metric].agg(['mean', 'std', 'count']).reset_index()
        
        # Calculate standard error
        grouped['se'] = grouped['std'] / np.sqrt(grouped['count'])
        
        if metric == 'different_trajectories_fraction':
            # Special handling for different_trajectories_fraction plot
            
            # Abbreviate category names
            grouped['abbreviated_tag'] = grouped['formatted_tag'].str.replace('Uncertainty Management', 'Uncertainty Mgmt.')
            grouped['abbreviated_tag'] = grouped['abbreviated_tag'].str.replace('Result Consolidation', 'Result Consol.')
            
            # Sort by mean value in descending order (highest fraction first)
            grouped = grouped.sort_values('mean', ascending=True)
            
            # Create figure
            plt.figure(figsize=FIGSIZE)
            
            # Create horizontal bar plot with error bars
            y_positions = range(len(grouped))
            bars = plt.barh(
                y_positions,
                grouped['mean'], 
                xerr=grouped['se'],
                capsize=5,
                alpha=0.8,
                edgecolor='black',
                linewidth=1
            )
            
            # Assign consistent colors to bars
            for i, bar in enumerate(bars):
                category = grouped['formatted_tag'].iloc[i]
                bar.set_color(CATEGORY_COLORS.get(category, '#7f7f7f'))
                
                # Add count annotation on the right of each bar
                count = grouped['count'].iloc[i]
                """
                plt.text(
                    bar.get_width() + grouped['se'].iloc[i] + 0.01,
                    bar.get_y() + bar.get_height()/2,
                    f'n={count}',
                    ha='left',
                    va='center',
                    fontsize=10
                )
                """
            
            # Set y-tick labels with colors and abbreviations
            plt.yticks(y_positions, grouped['abbreviated_tag'])
            
            # Color the y-tick labels to match the bars
            ax = plt.gca()
            for i, tick in enumerate(ax.get_yticklabels()):
                category = grouped['formatted_tag'].iloc[i]
                color = CATEGORY_COLORS.get(category, '#7f7f7f')
                tick.set_color(color)
            
            # Set title (no x-label, no y-label as requested)
            plt.title('Fraction of semantically different resampled sentences')
            
        else:
            # Regular handling for other metrics (overdeterminedness)
            
            # Sort by mean value in descending order
            grouped = grouped.sort_values('mean', ascending=False)
            
            # Create figure
            plt.figure(figsize=FIGSIZE)
            
            # Create bar plot with error bars
            bars = plt.bar(
                grouped['formatted_tag'], 
                grouped['mean'], 
                yerr=grouped['se'],
                capsize=5,
                alpha=0.8,
                edgecolor='black',
                linewidth=1
            )
            
            # Assign consistent colors to bars
            for i, bar in enumerate(bars):
                category = grouped['formatted_tag'].iloc[i]
                bar.set_color(CATEGORY_COLORS.get(category, '#7f7f7f'))
                
                # Add count annotation on top of each bar
                count = grouped['count'].iloc[i]
                plt.text(
                    bar.get_x() + bar.get_width()/2,
                    bar.get_height() + grouped['se'].iloc[i] + 0.01,
                    f'n={count}',
                    ha='center',
                    va='bottom',
                    fontsize=10
                )
            
            # Set labels and title
            plt.ylabel('Average Overdeterminedness')
            plt.title('Average Overdeterminedness by Category')
            plt.xlabel('Category')
            
            # Adjust x-axis labels
            plt.xticks(rotation=45, ha='right')
        
        # Add grid lines
        # plt.grid(axis='y', alpha=0.3, linestyle='--')
        
        # Remove top and right spines
        plt.gca().spines['top'].set_visible(False)
        plt.gca().spines['right'].set_visible(False)
        
        # Adjust layout
        plt.tight_layout()
        
        # Save plot
        display_metric = metric.replace('_', '_')
        plt.savefig(os.path.join(output_dir, f"avg_{display_metric}_by_category.png"), dpi=300)
        plt.close()
        
        print(f"Created average {metric} plot")
    
    print(f"Category average metrics plots saved to {output_dir}")

def plot_kl_metric_comparison_matrix(df_tags, output_dir="analysis/importance_plots"):
    """
    Create a plot with three subplots comparing all KL importance metrics against each other.
    All axes are normalized to 0-1 range for better readability.
    
    Args:
        df_tags: DataFrame with raw tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a copy of the dataframe
    df = df_tags.copy()
    
    # Format tag names for nicer display
    df['formatted_tag'] = df['tag'].apply(
        lambda x: ' '.join(word.capitalize() for word in x.split('_'))
    )
    
    # Filter out unwanted categories
    df = df[~df['formatted_tag'].isin(categories_to_exclude)]
    
    # Calculate overdeterminedness and filter out points with overdeterminedness > 0.9
    df['overdeterminedness'] = 1 - df['different_trajectories_fraction']
    df = df[df['overdeterminedness'] <= 0.95]
    
    # Define the KL metrics with their proper display names
    kl_metrics = {
        'resampling_importance_kl': 'Resampling Importance (KL)',
        'counterfactual_importance_kl': 'Counterfactual Importance (KL)',
        'forced_importance_kl': 'Forced Importance (KL)'
    }
    
    # Normalize each KL metric to 0-1 range
    for metric in kl_metrics.keys():
        if metric in df.columns:
            # Skip if no valid values
            if df[metric].notna().sum() == 0:
                continue
                
            # Create normalized version
            min_val = df[metric].min()
            max_val = df[metric].max()
            
            if max_val > min_val:
                df[f'{metric}_norm'] = (df[metric] - min_val) / (max_val - min_val)
            else:
                df[f'{metric}_norm'] = 0.5  # Default if all values are the same
    
    # Define the metric pairs to plot (now using normalized versions)
    metric_pairs = [
        ('resampling_importance_kl_norm', 'counterfactual_importance_kl_norm'),  # Resampling vs Counterfactual
        ('resampling_importance_kl_norm', 'forced_importance_kl_norm'),          # Resampling vs Forced
        ('counterfactual_importance_kl_norm', 'forced_importance_kl_norm')       # Counterfactual vs Forced
    ]
    
    # Get the original metric names for labels
    orig_metrics = {
        'counterfactual_importance_kl_norm': 'counterfactual_importance_kl',
        'resampling_importance_kl_norm': 'resampling_importance_kl',
        'forced_importance_kl_norm': 'forced_importance_kl'
    }
    
    # Use our consistent color palette for categories
    all_categories = sorted(df['formatted_tag'].unique())
    category_colors = {cat: CATEGORY_COLORS.get(cat, '#7f7f7f') for cat in all_categories}
    
    print("Generating normalized KL metric comparison matrix plot...")
    
    # Create figure with 1x3 subplots
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    fig.suptitle("Comparisons of KL-based Importance Metrics (Normalized 0-1 Scale)", fontsize=16)
    
    # For each metric pair, create a subplot
    for i, (x_metric, y_metric) in enumerate(metric_pairs):
        ax = axes[i]
        
        # Get original metric names for labels
        x_orig = orig_metrics[x_metric]
        y_orig = orig_metrics[y_metric]
        
        # Filter data where both metrics are available
        df_pair = df.dropna(subset=[x_metric, y_metric])
        
        # Skip if not enough data
        if len(df_pair) < 3:
            ax.text(0.5, 0.5, f"Insufficient data", 
                   horizontalalignment='center', verticalalignment='center',
                   transform=ax.transAxes)
            continue
        
        # Create scatter plot, color points by category
        for category in all_categories:
            cat_data = df_pair[df_pair['formatted_tag'] == category]
            if len(cat_data) < 3:
                continue
                
            ax.scatter(
                cat_data[x_metric], 
                cat_data[y_metric],
                alpha=0.7,
                s=30,
                label=category,
                color=category_colors[category]
            )
        
        # Add y=x line
        ax.plot([0, 1], [0, 1], color='#AAAAAA', linestyle='--', linewidth=2)
        
        # Set labels and title
        ax.set_xlabel(kl_metrics[x_orig])
        ax.set_ylabel(kl_metrics[y_orig])
        ax.set_title(f"{kl_metrics[x_orig].split()[0]} vs {kl_metrics[y_orig].split()[0]}")
        
        # Set fixed axis limits for 0-1 range
        ax.set_xlim(-0.05, 1.05)
        ax.set_ylim(-0.05, 1.05)
        
        # Ensure same ticks on both axes
        ticks = np.arange(0, 1.1, 0.2)
        ax.set_xticks(ticks)
        ax.set_yticks(ticks)
        
        # Remove top and right spines
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
    
    # Add a single legend for the entire figure
    handles, labels = axes[0].get_legend_handles_labels()
    fig.legend(handles, labels, loc='lower center', ncol=len(all_categories)+1, bbox_to_anchor=(0.5, 0), fontsize=10)
    
    # Adjust spacing
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.2)  # Make room for the legend
    
    # Save figure
    plot_filename = "kl_metrics_comparison_matrix_normalized.png"
    plt.savefig(os.path.join(output_dir, plot_filename), dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Saved normalized KL metric comparison matrix to {plot_filename}")

def plot_kl_metric_comparison_by_category(df_tags, output_dir="analysis/importance_plots"):
    """
    Create three separate plots, one for each KL metric pair, where each plot has subplots for different categories.
    All axes are normalized to 0-1 range for better readability.
    
    Args:
        df_tags: DataFrame with raw tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a copy of the dataframe
    df = df_tags.copy()
    
    # Format tag names for nicer display
    df['formatted_tag'] = df['tag'].apply(
        lambda x: ' '.join(word.capitalize() for word in x.split('_'))
    )
    
    # Filter out unwanted categories
    df = df[~df['formatted_tag'].isin(categories_to_exclude)]
    
    # Calculate overdeterminedness and filter out points with overdeterminedness > 0.9
    df['overdeterminedness'] = 1 - df['different_trajectories_fraction']
    df = df[df['overdeterminedness'] <= 0.95]
    
    # Define the KL metrics with their proper display names
    kl_metrics = {
        'resampling_importance_kl': 'Resampling Importance (KL)',
        'counterfactual_importance_kl': 'Counterfactual Importance (KL)',
        'forced_importance_kl': 'Forced Importance (KL)'
    }
    
    # Normalize each KL metric to 0-1 range using a more robust approach
    for metric in kl_metrics.keys():
        if metric in df.columns:
            # Skip if no valid values
            if df[metric].notna().sum() == 0:
                continue
                
            # Get valid values only
            valid_values = df[metric].dropna()
            
            # Use percentiles to handle outliers
            min_val = valid_values.quantile(0.01)  # 1st percentile
            max_val = valid_values.quantile(0.99)  # 99th percentile
            
            # df = df[df[metric] < 1.0]
            
            # Create normalized version with clipping to handle outliers
            df[f'{metric}_norm'] = df[metric] # ((df[metric] - min_val) / (max_val - min_val)).clip(0, 1)
            
            # Create normalized version
            """
            min_val = df[metric].min()
            max_val = df[metric].max()
            
            if max_val > min_val:
                df[f'{metric}_norm'] = (df[metric] - min_val) / (max_val - min_val)
            else:
                df[f'{metric}_norm'] = 0.5  # Default if all values are the same
            """
    
    # Define the metric pairs to plot (now using normalized versions)
    metric_pairs = [
        ('resampling_importance_kl_norm', 'counterfactual_importance_kl_norm'),  # Resampling vs Counterfactual
        ('resampling_importance_kl_norm', 'forced_importance_kl_norm'),          # Resampling vs Forced
        ('counterfactual_importance_kl_norm', 'forced_importance_kl_norm')       # Counterfactual vs Forced
    ]
    
    # Get the original metric names for labels
    orig_metrics = {
        'counterfactual_importance_kl_norm': 'counterfactual_importance_kl',
        'resampling_importance_kl_norm': 'resampling_importance_kl',
        'forced_importance_kl_norm': 'forced_importance_kl'
    }
    
    # Get unique categories
    categories = sorted(df['formatted_tag'].unique())
    categories = ['Active Computation', 'Fact Retrieval', 'Plan Generation', 'Uncertainty Management']
    
    # Calculate grid dimensions for subplots
    n_categories = len(categories)
    n_cols = 4 # min(3, n_categories)
    n_rows = 1 # (n_categories + n_cols - 1) // n_cols
    
    print("Generating normalized KL metric comparison plots by category...")
    
    # Create a separate plot for each metric pair
    for x_metric, y_metric in metric_pairs:
        # Get the original metric names for labels
        x_orig = orig_metrics[x_metric]
        y_orig = orig_metrics[y_metric]
        
        # Get the display names for this pair
        x_display = kl_metrics[x_orig]
        y_display = kl_metrics[y_orig]
        
        # Create figure with subplots for each category
        fig = plt.figure(figsize=(6*n_cols, 5*n_rows))
        fig.suptitle(f"{y_display.split()[0]} vs. {x_display.split()[0].lower()} importance by category", fontsize=32, y=1.00) # (Normalized 0-1 Scale)
        
        # For each category, create a subplot
        for idx, category in enumerate(categories, 1):
            ax = fig.add_subplot(n_rows, n_cols, idx)
            
            # Filter data for this category
            cat_data = df[df['formatted_tag'] == category].dropna(subset=[x_metric, y_metric])
            
            # Skip if not enough data
            if len(cat_data) < 3:
                ax.text(0.5, 0.5, f"Insufficient data for\n{category}", 
                      horizontalalignment='center', verticalalignment='center',
                      transform=ax.transAxes)
                ax.set_xticks([])
                ax.set_yticks([])
                continue
            
            # Get category color
            cat_color = CATEGORY_COLORS.get(category, '#7f7f7f')
            
            # Create scatter plot
            ax.scatter(
                cat_data[x_metric], 
                cat_data[y_metric],
                alpha=0.7,
                s=30,
                color=cat_color
            )
            
            # Add y=x line instead of best fit
            ax.plot([0, 1], [0, 1], color='#AAAAAA', linestyle='--', linewidth=2)
            
            # Set title for this subplot
            ax.set_title(category, color=cat_color)
            
            # Set axis labels
            if 'active' in category.lower() or 'fact' in category.lower() or 'plan' in category.lower() or 'result' in category.lower() or 'uncertainty' in category.lower():
                ax.set_xlabel(x_display.replace('Importance (KL)', '(KL)'))
            if 'active' in category.lower():
                ax.set_ylabel(y_display.replace('Importance (KL)', '(KL)'))
            
            
            # Set fixed axis limits for 0-1 range
            ax.set_xlim(-0.05, 1.05)
            ax.set_ylim(-0.05, 1.05)
            
            # Ensure same ticks on both axes
            ticks = np.arange(0, 1.1, 0.2)
            ax.set_xticks(ticks)
            ax.set_yticks(ticks)
            
            # Remove top and right spines
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
        
        # Adjust spacing
        plt.tight_layout()
        
        # Save figure
        pair_name = f"{x_orig.split('_')[0]}_{y_orig.split('_')[0]}"
        plot_filename = f"kl_metrics_{pair_name}_by_category_normalized.png"
        plt.savefig(os.path.join(output_dir, plot_filename), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Saved normalized {x_display.split()[0]} vs {y_display.split()[0]} comparison by category to {plot_filename}")
    
    print("Normalized KL metric comparison plots by category complete.")

def plot_category_vs_overdeterminedness(df_tags, output_dir="analysis/importance_plots"):
    """
    Create a plot with subplots showing the relationship between each category and overdeterminedness.
    Each category has its own subplot.
    
    Args:
        df_tags: DataFrame with raw tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a copy of the dataframe
    df = df_tags.copy()
    
    # Format tag names for nicer display
    df['formatted_tag'] = df['tag'].apply(
        lambda x: ' '.join(word.capitalize() for word in x.split('_'))
    )
    
    # Filter out unwanted categories
    df = df[~df['formatted_tag'].isin(categories_to_exclude)]
    
    # Calculate overdeterminedness and filter out points with overdeterminedness > 0.95
    df['overdeterminedness'] = 1 - df['different_trajectories_fraction']
    df = df[df['overdeterminedness'] <= 0.95]
    
    # Get unique categories
    categories = sorted(df['formatted_tag'].unique())
    
    # Calculate grid dimensions for subplots
    n_categories = len(categories)
    n_cols = min(3, n_categories)
    n_rows = (n_categories + n_cols - 1) // n_cols
    
    print("Generating category vs. overdeterminedness plots...")
    
    # Create figure with subplots
    fig = plt.figure(figsize=(6*n_cols, 5*n_rows))
    fig.suptitle("Category vs. Overdeterminedness", fontsize=16, y=1.02)
    
    # For each category, create a subplot
    for idx, category in enumerate(categories, 1):
        ax = fig.add_subplot(n_rows, n_cols, idx)
        
        # Filter data for this category
        cat_data = df[df['formatted_tag'] == category]
        
        # Skip if not enough data
        if len(cat_data) < 3:
            ax.text(0.5, 0.5, f"Insufficient data for\n{category}", 
                  horizontalalignment='center', verticalalignment='center',
                  transform=ax.transAxes)
            ax.set_xticks([])
            ax.set_yticks([])
            continue
        
        # Get category color
        cat_color = CATEGORY_COLORS.get(category, '#7f7f7f')
        
        # Create scatter plot of normalized position vs overdeterminedness
        ax.scatter(
            cat_data['normalized_position'],
            cat_data['overdeterminedness'],
            alpha=0.7,
            s=30,
            color=cat_color
        )
        
        # Set title for this subplot
        ax.set_title(category, fontsize=12)
        
        # Set axis labels
        ax.set_xlabel("Normalized Position", fontsize=10)
        ax.set_ylabel("Overdeterminedness", fontsize=10)
        
        # Set axis limits
        ax.set_xlim(-0.05, 1.05)
        ax.set_ylim(-0.05, 1.05)
        
        # Remove top and right spines
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
    
    # Adjust spacing
    plt.tight_layout()
    
    # Save figure
    plot_filename = "category_vs_overdeterminedness.png"
    plt.savefig(os.path.join(output_dir, plot_filename), dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Saved category vs. overdeterminedness plots to {plot_filename}")

def plot_category_frequencies(df_tags, output_dir="analysis/importance_plots"):
    """
    Create a bar chart showing the frequency/count of each reasoning category.
    
    Args:
        df_tags: DataFrame with raw tag data
        output_dir: Directory to save plots
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create a copy of the dataframe
    df = df_tags.copy()
    
    # Format tag names for nicer display
    df['formatted_tag'] = df['tag'].apply(
        lambda x: ' '.join(word.capitalize() for word in x.split('_'))
    )
    
    # Replace "Uncertainty Management" with "Uncertainty Mgmt."
    df['formatted_tag'] = df['formatted_tag'].str.replace('Uncertainty Management', 'Uncertainty Mgmt.')
    
    # Count the frequency of each category
    category_counts = df['formatted_tag'].value_counts().reset_index()
    category_counts.columns = ['Category', 'Count']
    
    # Sort by count in descending order
    category_counts = category_counts.sort_values('Count', ascending=False)
    
    # Calculate percentage for each category
    total_count = category_counts['Count'].sum()
    category_counts['Percentage'] = (category_counts['Count'] / total_count * 100).round(1)
    
    # Create formatted category labels with line breaks for the last word
    formatted_labels = []
    for category in category_counts['Category']:
        words = category.split()
        if len(words) > 1:
            # Put the last word on a new line
            formatted_label = ' '.join(words[:-1]) + '\n' + words[-1]
        else:
            formatted_label = category
        formatted_labels.append(formatted_label)
    
    # Create figure
    plt.figure(figsize=FIGSIZE)
    
    # Create bar chart
    bars = plt.bar(
        range(len(category_counts)),  # Use numerical x positions
        category_counts['Count'],
        alpha=0.8,
        edgecolor='black',
        linewidth=1
    )
    
    # Assign the correct colors to each bar based on category and add percentage labels
    for i, bar in enumerate(bars):
        category = category_counts['Category'].iloc[i]
        color = CATEGORY_COLORS.get(category.replace('Uncertainty Mgmt.', 'Uncertainty Management'), '#7f7f7f')  # Default to gray if category not found
        bar.set_color(color)
        
        # Add percentage label on top of each bar
        percentage = category_counts['Percentage'].iloc[i]
        plt.text(
            bar.get_x() + bar.get_width()/2,
            bar.get_height() + 5,
            f'{percentage}%',
            ha='center',
            va='bottom',
            fontsize=FONT_SIZE - 2
        )
    
    # Set x-tick positions and labels with the same colors as the bars
    plt.xticks(range(len(category_counts)), formatted_labels)
    
    # Color the x-tick labels to match the bars
    ax = plt.gca()
    for i, tick in enumerate(ax.get_xticklabels()):
        category = category_counts['Category'].iloc[i]
        color = CATEGORY_COLORS.get(category.replace('Uncertainty Mgmt.', 'Uncertainty Management'), '#7f7f7f')
        tick.set_color(color)
    
    # Set labels and title
    # Remove Category label
    plt.ylabel("Count")
    plt.title("Distribution of sentence categories")
    
    # Make x-axis labels horizontal
    plt.xticks(rotation=0)
    
    # Remove top and right spines
    plt.gca().spines['top'].set_visible(False)
    plt.gca().spines['right'].set_visible(False)
    
    # Add tight layout
    plt.tight_layout()
    
    # Save plot
    plt.savefig(os.path.join(output_dir, "category_frequencies.png"), dpi=300)
    plt.close()
    
    print("Created category frequencies plot")

def filter_problems_by_min_steps(df_tags, min_steps):
    """
    Filter out traces (pn_ci) that have fewer than min_steps steps (chunks).
    Returns filtered DataFrame and stats on omitted traces.
    """
    # Count number of chunks per trace (pn_ci)
    trace_chunk_counts = df_tags.groupby('pn_ci').size()
    traces_with_enough_steps = trace_chunk_counts[trace_chunk_counts >= min_steps].index
    filtered_df = df_tags[df_tags['pn_ci'].isin(traces_with_enough_steps)].copy()
    num_omitted = df_tags['pn_ci'].nunique() - len(traces_with_enough_steps)
    print(f"Omitted {num_omitted} traces with fewer than {min_steps} steps. Remaining: {len(traces_with_enough_steps)} traces.")
    return filtered_df

def main():
    """Main function to process data and generate plots."""
    # Set paths
    rollouts_dir = f"math_rollouts/deepseek-r1-distill-{args.model}/temperature_0.6_top_p_0.95"
    output_dir = "analysis/importance_plots" if args.model == 'qwen-14b' else "analysis/importance_plots_llama-8b"
    
    # Collect tag data from chunks_labeled.json files
    df_tags = collect_tag_data(rollouts_dir)
    
    if df_tags is None or df_tags.empty:
        print("No tag data found, exiting")
        return
    
    # Filter out problems with fewer than min_steps steps
    df_tags = filter_problems_by_min_steps(df_tags, args.min_steps)
    
    # Plot category frequencies
    print("Generating category frequencies plot...")
    plot_category_frequencies(df_tags, output_dir)
    
    # Aggregate tag data
    print("Aggregating tag data...")
    df_agg = aggregate_tag_data(df_tags)
    
    # Generate regular plots
    print("Generating regular plots...")
    plot_tag_effects(df_agg, output_dir)
    
    # Generate position-debiased plots if scikit-learn is available
    if HAS_SKLEARN:
        print("Generating position-debiased plots...")
        plot_debiased_tag_effects(df_tags, output_dir)
    
    # Generate KL metric comparison plots
    print("Generating KL metric comparison plots...")
    plot_kl_metric_comparison(df_tags, output_dir)
    
    # Generate individual points plots
    print("Generating individual points plots...")
    plot_individual_points_by_category(df_tags, output_dir)
    
    # Generate overdeterminedness comparison plots
    print("Generating overdeterminedness comparison plots...")
    plot_overdeterminedness_comparison(df_tags, output_dir)
    
    # Plot category subplots
    plot_overdeterminedness_category_subplots(df_tags, output_dir)
    
    # Plot average metrics by category
    print("Generating average metrics by category plots...")
    plot_category_metrics_averages(df_tags, output_dir)
    
    # Plot KL metric comparison matrix
    print("Generating KL metric comparison matrix plot...")
    plot_kl_metric_comparison_matrix(df_tags, output_dir)
    
    # Plot KL metric comparison by category
    print("Generating KL metric comparison plots by category...")
    plot_kl_metric_comparison_by_category(df_tags, output_dir)
    
    # Plot category vs. overdeterminedness
    print("Generating category vs. overdeterminedness plots...")
    plot_category_vs_overdeterminedness(df_tags, output_dir)
    
    # Save aggregated data
    agg_output = os.path.join(output_dir, "aggregated_tag_data.csv")
    df_agg.to_csv(agg_output, index=False)
    print(f"Saved aggregated tag data to {agg_output}")
    
    print("Done!")

if __name__ == "__main__":
    main()
